{
  "0\nInternational Software Testing Qualifications Board\nProvided by\nAlliance for Qualification": "0\nInternational Software Testing Qualifications Board\nProvided by\nAlliance for Qualification, Artificial Intelligence United,\nChinese Software Testing Qualifications Board,\nand Korean Software Testing Qualifications Board\nCertified Tester\nAI Testing (CT-AI)\nSyllabus\nCopyright Notice\nCopyright Notice \u00a9 International Software Testing Qualifications Board (hereinafter called ISTQB\u00ae)\nISTQB\u00ae is a registered trademark of the International Software Testing Qualifications Board.\nCopyright \u00a9 2021, the authors Klaudia Dussa-Zieger (chair), Werner Henschelchen, Vipul Kocher,\nQin Liu, Stuart Reid, Kyle Siemens, and Adam Leon Smith.\nAll rights reserved. The authors hereby transfer the copyright to the ISTQB\u00ae. The authors (as current\ncopyright holders) and ISTQB\u00ae (as the future copyright holder) have agreed to the following\nconditions of use:\nExtracts, for non-commercial use, from this document may be copied if the source is acknowledged.\nAny Accredited Training Provider may use this syllabus as the basis for a training course if the\nauthors and the ISTQB\u00ae are acknowledged as the source and copyright owners of the syllabus and\nprovided that any advertisement of such a training course may mention the syllabus only after official\nAccreditation of the training materials has been received from an ISTQB\u00ae-recognized Member Board.\nAny individual or group of individuals may use this syllabus as the basis for articles and books, if the\nauthors and the ISTQB\u00ae are acknowledged as the source and copyright owners of the syllabus.\nAny other use of this syllabus is prohibited without first obtaining the approval in writing of the\nISTQB\u00ae.\nAny ISTQB\u00ae-recognized Member Board may translate this syllabus provided they reproduce the\nabovementioned Copyright Notice in the translated version of the syllabus.\nv1.",
  "0 Page ": "0 Page 99 of 99 2021-10-01\n\u00a9 International Software Testing Qualifications Board\n",
  "01 Release for GA\nv": "01 Release for GA\nv1.",
  "2\nRevision History ": "2\nRevision History ..................................................................................................................................... ",
  "3\nTable of Contents ": "3\nTable of Contents .................................................................................................................................. ",
  "4\nAcknowledgements ": "4\nAcknowledgements ................................................................................................................................ 8\n",
  "0 Introduction ": "0 Introduction ................................................................................................................................... 9\n0.",
  "1 Purpose of this Syllabus ": "1 Purpose of this Syllabus ....................................................................................................... 9\n0.",
  "2 The Certified Tester AI Testing ": "2 The Certified Tester AI Testing ............................................................................................. 9\n0.",
  "3 Examinable Learning Objectives and Cognitive Level of Knowledge ": "3 Examinable Learning Objectives and Cognitive Level of Knowledge ................................... 9\n0.",
  "4 Hands": "4 Hands-on Levels of Competency\nThe Certified Tester Specialist AI Testing includes hands-on objectives which focus on practical skills\nand competencies.\nThe following levels apply to hands-on objectives (as shown):\n\u2022 H0: Live demo of an exercise or recorded video.\n\u2022 H1: Guided exercise. The students follow a sequence of steps performed by the trainer.\n\u2022 H2: Exercise with hints. The student is given an exercise with relevant hints so the exercise\ncan be solved within the given timeframe, or students take part in a discussion.\nCompetencies are achieved by performing hands-on exercises, such shown in the following list:\n\u2022 Demonstrate underfitting and overfitting (H0).\n\u2022 Perform data preparation in support of the creation of an ML model (H2).\n\u2022 Identify training and test datasets and create an ML model (H2).\n\u2022 Evaluate the created ML model using selected ML functional performance metrics (H2).\n\u2022 Experience of the implementation of a perceptron (H1).\n\u2022 Use of a tool to show how explainability can be used by testers (H2).\n\u2022 Apply pairwise testing to derive and execute test cases for an AI-based system (H2).\n\u2022 Apply metamorphic testing to derive and execute test cases for a given scenario (H2).\n\u2022 Apply exploratory testing to an AI-based system (H2).\n\u2022 Discuss, using examples, those activities in testing where AI is less likely to be used (H2).\n\u2022 Implement a simple AI-based defect prediction system (H2).\n0.",
  "5 The Certified Tester AI Testing Exam ": "5 The Certified Tester AI Testing Exam ................................................................................ 10\n0.",
  "6 Accreditation ": "6 Accreditation ....................................................................................................................... 11\n0.",
  "7 Level of Detail ": "7 Level of Detail ..................................................................................................................... 11\n0.",
  "8 How this Syllabus is Organized ": "8 How this Syllabus is Organized .......................................................................................... 11\n",
  "1 Introduction to AI ": "1 Introduction to AI \u2013 105 minutes\nTesting Keywords\nNone\nAI-Specific Keywords\nAI as a Service (AIaaS), AI development framework, AI effect, AI-based system, artificial intelligence\n(AI), neural network, deep learning (DL), deep neural network, general AI, General Data Protection\nRegulation (GDPR), machine learning (ML), narrow AI, pre-trained model, super AI, technological\nsingularity, transfer learning\nLearning Objectives for Chapter 1:\n1.",
  "1 Definition of AI and AI Effect ": "1 Definition of AI and AI Effect ............................................................................................... 13\n1.",
  "2 Narrow": "2 Narrow, General and Super AI\nAt a high level, AI can be broken into three categories:\n\u2022 Narrow AI (also known as weak AI) systems are programmed to carry out a specific task with\nlimited context. Currently this form of AI is widely available. For example, game-playing\nsystems, spam filters, test case generators and voice assistants.\n\u2022 General AI (also known as strong AI) systems have general (wide-ranging) cognitive abilities\nsimilar to humans. These AI-based systems can reason and understand their environment as\nhumans do, and act accordingly. As of 2021, no general AI systems have been realized.\n\u2022 Super AI systems are capable of replicating human cognition (general AI) and make use of\nmassive processing power, practically unlimited memory and access to all human knowledge\n(e.g., through access to the web). It is thought that super AI systems will quickly become\nwiser than humans. The point at which AI-based systems transition from general AI to super\nAI is commonly known as the technological singularity [B01].\n1.",
  "3 AI": "3 AI-Based and Conventional Systems\nIn a typical conventional computer system, the software is programmed by humans using an\nimperative language, which includes constructs such as if-then-else and loops. It is relatively easy for\nhumans to understand how the system transforms inputs into outputs. In an AI-based system using\nmachine learning (ML), patterns in data are used by the system to determine how it should react in\nthe future to new data (see Chapter 3 for a detailed explanation of ML). For example, an AI-based\nimage processor designed to identify images of cats is trained with a set of images known to contain\ncats. The AI determines on its own what patterns or features in the data can be used to identify cats.\nThese patterns and rules are then applied to new images in order to determine if they contain cats. In\nmany AI-based systems, this results in the prediction-making procedure being less easy to\nunderstand by humans (see Section 2.7).\nv1.",
  "4 AI Technologies ": "4 AI Technologies .................................................................................................................. 14\n1.",
  "5 AI Development Frameworks ": "5 AI Development Frameworks ............................................................................................. 14\n1.",
  "6 Hardware for AI": "6 Hardware for AI-Based Systems\nA variety of hardware is used for ML model training (see Chapter 3) and model implementation. For\nexample, a model that performs speech recognition may run on a low-end smartphone, although\naccess to the power of cloud computing may be needed to train it. A common approach used when\nthe host device is not connected to the internet is to train the model in the cloud and then deploy it to\nthe host device.\nML typically benefits from hardware that supports the following attributes:\n\u2022 Low-precision arithmetic: This uses fewer bits for computation (e.g., 8 instead of 32 bits,\nwhich is usually all that is needed for ML).\n\u2022 The ability to work with large data structures (e.g., to support matrix multiplication).\n\u2022 Massively parallel (concurrent) processing.\nGeneral-purpose CPUs provide support for complex operations that are not typically required for ML\napplications and only provide a few cores. As a result, their architecture is less efficient for training\nand running ML models when compared to GPUs, which have thousands of cores and which are\ndesigned to perform the massively parallel but relatively simple processing of images. As a\nconsequence, GPUs typically outperform CPUs for ML applications, even though CPUs typically have\nfaster clock speeds. For small-scale ML work, GPUs generally offer the best option.\nSome hardware is specially intended for AI, such as purpose-built Application-Specific Integrated\nCircuits (ASICs) and System on a Chip (SoC). These AI-specific solutions have features such as\nmultiple cores, special data management and the ability to perform in-memory processing. They are\nmost suitable for edge computing, while the training of the ML model is done in the cloud.\nHardware with specific AI architectures is currently (as of April 2021) under development. This\nincludes neuromorphic processors [B03], which do not use the traditional von Neumann architecture,\nbut rather an architecture that loosely mimics brain neurons.\nExamples of AI hardware providers and their processors include (as of April 2021):\n\u2022 NVIDIA: They provide a range of GPUs and AI-specific processors, such as the Volta [R09].\n\u2022 Google: They have developed application-specific integrated circuits for both training and\ninferencing. Google TPUs (Cloud Tensor Processing Units) [R10] can be accessed by users\non the Google Cloud, whereas the Edge TPU [R11] is a purpose-built ASIC designed to run\nAI on individual devices.\nv1.",
  "7 AI as a Service ": "7 AI as a Service (AIaaS)\nAI components, such as ML models, can be created within an organization, downloaded from a third-\nparty, or used as a service on the web (AIaaS). A hybrid approach is also possible in which some of\nthe AI functionality is provided from within the system and some is provided as a service.\nWhen ML is used as a service, access is provided to an ML model over the web and support can also\nbe provided for data preparation and storage, model training, evaluation, tuning, testing and\ndeployment.\nThird-party providers (e.g., AWS, Microsoft) offer specific AI services, such as facial and speech\nrecognition. This allows individuals and organizations to implement AI using cloud-based services\neven when they have insufficient resources and expertise to build their own AI services. In addition,\nML models provided as part of a third-party service are likely to have been trained on a larger, more\ndiverse training dataset than is readily available to many stakeholders, such as those who have\nrecently moved into the AI market.\n1.7.",
  "1 Contracts for AI as a Service": "1 Contracts for AI as a Service.......................................................................................... 16\n1.7.",
  "2 AIaaS Examples ": "2 AIaaS Examples ............................................................................................................. 16\n1.",
  "8 Pre": "8 Pre-Trained Models\n1.8.",
  "1 Introduction to Pre": "1 Introduction to Pre-Trained Models\nIt can be expensive to train ML models (see Chapter 3). First, the data has to be prepared and then\nthe model must be trained. The first activity can consume large amounts of human resources, while\nthe latter activity can consume a lot of computing resources. Many organizations do not have access\nto these resources.\nA cheaper and often more effective alternative is to use a pre-trained model. This provides similar\nfunctionality to the required model and is used as the basis for creating a new model that extends\nand/or focuses the functionality of the pre-trained model. Such models are only available for a limited\nnumber of technologies, such as neural networks and random forests.\nIf an image classifier is needed, it could be trained using the publicly available ImageNet dataset,\nwhich contains over 14 million images classified into over 1000 categories. This reduces the risk of\nconsuming significant resources with no guarantee of success. Alternatively, an existing model could\nbe reused that has already been trained on this dataset. By using such a pre-trained model, training\ncosts are saved and the risk of it not working largely eliminated.\nWhen a pre-trained model is used without modification, it can simply be embedded in the AI-based\nsystem, or it can be used as a service (see Section 1.7).\n1.8.",
  "2 Transfer Learning ": "2 Transfer Learning ........................................................................................................... 17\n1.8.",
  "3 Risks of using Pre": "3 Risks of using Pre-Trained Models and Transfer Learning\nUsing pre-trained models and transfer learning are both common approaches to building AI-based\nsystems, but there are some risks associated. These include:\n\u2022 A pre-trained model may lack transparency compared to an internally generated model.\n\u2022 The level of similarity between the function performed by the pre-trained model and the\nrequired functionality may be insufficient. Also, this difference may not be understood by the\ndata scientist.\n\u2022 Differences in the data preparation steps (see Section 4.1) used for the pre-trained model\nwhen originally developed and the data preparation steps used when this model is then used\nin a new system may impact the resulting functional performance.\n\u2022 The shortcomings of a pre-trained model are likely to be inherited by those who reuse it and\nmay not be documented. For example, inherited biases (see Section 2.4) may not be\napparent if there is a lack of documentation about the data used to train the model. Also, if\nthe pre-trained model is not widely used, there are likely to be more unknown (or\nundocumented) defects and more rigorous testing may be needed to mitigate this risk.\n\u2022 Models created through transfer learning are highly likely to be sensitive to the same\nvulnerabilities as the pre-trained model on which it is based (e.g., adversarial attacks, as\nexplained in 9.1.1). In addition, if an AI-based system is known to contain a specific pre-\ntrained model (or is based on a specific pre-trained model), then vulnerabilities associated\nwith it may already be known by potential attackers.\nNote that several of the above risks can be more easily mitigated by having thorough documentation\navailable for the pre-trained model (see Section 7.5).\n1.",
  "9 Standards": "9 Standards, Regulations and AI\nThe Joint Technical Committee of IEC and ISO on information technology (ISO/IEC JTC1) prepares\ninternational standards which contribute towards AI. For example, a subcommittee on AI (ISO/IEC\nJTC 1/SC42), was set up in ",
  "2 Quality Characteristics for AI": "2 Quality Characteristics for AI-Based Systems \u2013 105\nminutes\nKeywords\nNone\nAI-Specific Keywords\nAdaptability, algorithmic bias, autonomy, bias, evolution, explainability, explainable AI (XAI), flexibility,\ninappropriate bias, interpretability, ML system, machine learning, reward hacking, robustness, sample\nbias, self-learning system, side effects, transparency\nLearning Objectives for Chapter 2:\n2.",
  "1 Flexibility and Adaptability ": "1 Flexibility and Adaptability .................................................................................................. 21\n2.",
  "2 Autonomy ": "2 Autonomy ........................................................................................................................... 21\n2.",
  "3 Evolution ": "3 Evolution ............................................................................................................................. 21\nv1.",
  "4 Bias ": "4 Bias ..................................................................................................................................... 22\n2.",
  "5 Ethics ": "5 Ethics .................................................................................................................................. 22\n2.",
  "6 Side Effects and Reward Hacking ": "6 Side Effects and Reward Hacking ...................................................................................... 23\n2.",
  "7 Transparency": "7 Transparency, Interpretability and Explainability\nAI-based systems are typically applied in areas where users need to trust those systems. This may be\nfor safety reasons, but also where privacy is needed and where they might provide potentially life-\nchanging predictions and decisions.\nMost users are presented with AI-based systems as \u201cblack boxes\u201d and have little awareness of how\nthese systems arrive at their results. In some cases, this ignorance may even apply to the data\nscientists who built the systems. Occasionally, users may not even be aware they are interacting with\nan AI-based system.\nv1.",
  "8 Safety and AI ": "8 Safety and AI ...................................................................................................................... 24\n",
  "3 Machine Learning ": "3 Machine Learning (ML) \u2013 Overview - 145 minutes\nKeywords\nNone\nAI-Specific Keywords\nAssociation, classification, clustering, data preparation, ML algorithm, ML framework, ML functional\nperformance criteria, ML model, ML training data, ML workflow, model evaluation, model tuning,\noutlier, overfitting, regression, reinforcement learning, supervised learning, underfitting, unsupervised\nlearning\nLearning Objectives for Chapter 3:\n3.",
  "1 Forms of ML ": "1 Forms of ML ....................................................................................................................... 26\n3.1.",
  "1 Supervised Learning ": "1 Supervised Learning ...................................................................................................... 26\n3.1.",
  "2 Unsupervised Learning ": "2 Unsupervised Learning .................................................................................................. 26\n3.1.",
  "3 Reinforcement Learning ": "3 Reinforcement Learning ................................................................................................. 27\n3.",
  "2 ML Workflow ": "2 ML Workflow ....................................................................................................................... 27\n3.",
  "3 Selecting a Form of ML ": "3 Selecting a Form of ML ...................................................................................................... 30\n3.",
  "4 Factors Involved in ML Algorithm Selection ": "4 Factors Involved in ML Algorithm Selection ....................................................................... 30\n3.",
  "5 Overfitting and Underfitting ": "5 Overfitting and Underfitting ................................................................................................. 31\n3.5.",
  "1 Overfitting ": "1 Overfitting ....................................................................................................................... 31\n3.5.",
  "2 Underfitting ": "2 Underfitting ..................................................................................................................... 31\n3.5.",
  "3 Hands": "3 Hands-On Exercise: Demonstrate Overfitting and Underfitting\nDemonstrate the concepts of overfitting and underfitting on a model. This could be demonstrated by\nusing a dataset that contains very little data (overfitting), and a dataset with poor feature correlations\n(underfitting).\nv1.",
  "4 ML ": "4 ML - Data \u2013 230 minutes\nKeywords\nNone\nAI-Specific Keywords\nAnnotation, augmentation, classification model, data labelling, data preparation, ML training data,\nsupervised learning, test dataset, validation dataset\nLearning Objectives for Chapter 4:\n4.",
  "1 Data Preparation as Part of the ML Workflow ": "1 Data Preparation as Part of the ML Workflow .................................................................... 33\n4.1.",
  "1 Challenges in Data Preparation ": "1 Challenges in Data Preparation ..................................................................................... 34\n4.1.",
  "2 Hands": "2 Hands-On Exercise: Data Preparation for ML\nFor a given set of raw data, perform the applicable data preparation steps as outlined in Section 4.1 to\nproduce a dataset that will be used to create a classification model using supervised learning.\nThis activity forms the first step in creating an ML model that will be used for future exercises.\nTo perform this activity, students will be provided with appropriate (and language-specific) materials,\nincluding:\n\u2022 Libraries\n\u2022 ML frameworks\n\u2022 Tools\n\u2022 A development environment\nv1.",
  "2 Training": "2 Training, Validation and Test Datasets in the ML Workflow\nLogically, three sets of equivalent data (i.e., randomly selected from a single initial dataset) are\nrequired to develop an ML model:\n\u2022 A training dataset, which is used to train the model.\n\u2022 A validation dataset, which used for evaluating and subsequently tuning the model.\n\u2022 A test dataset, (also known as the holdout dataset), which is used for testing the tuned model.\nIf unlimited suitable data is available, the amount of data used in the ML workflow for training,\nevaluation and testing typically depends on the following factors:\n\u2022 The algorithm used to train the model.\n\u2022 The availability of resources, such as RAM, disk space, computing power, network bandwidth\nand the available time.\nIn practice, due to the challenge of acquiring sufficient suitable data, the training and validation\ndatasets are often derived from a single combined dataset. The test dataset is kept separate and is\nnot used during training. This is to ensure the developed model is not influenced by the test data, and\nso that test results give a true reflection of the model\u2019s quality\nThere is no optimal ratio for splitting the combined dataset into the three individual datasets, but the\ntypical ratios which may be used as a guideline range from 60:20:20 to 80:10:10 (training: validation:\ntest). Splitting the data into these datasets it is often done randomly, unless the dataset is small or if\nthere is a risk of the resultant datasets not being representative of the expected operational data.\nIf limited data is available, then splitting the available data into three datasets may result in insufficient\ndata being available for effective training. To overcome this issue, the training and validation datasets\nmay be combined (keeping the test dataset separate), and then used to create multiple split\ncombinations of this dataset (e.g., 80% training / 20% validation). Data is then randomly assigned to\nthe training and validation datasets. Training, validation and tuning are performed using these multiple\nsplit combinations to create multiple tuned models, and the overall model performance may be\ncalculated as the average across all runs. There are various methods used for creating multiple split\ncombinations, which include split-test, bootstrap, K-fold cross validation and leave-one-out cross\nvalidation (see [B02] for more details).\n4.2.",
  "1 Hands": "1 Hands-On Exercise: Build a Defect Prediction System\nStudents will use a suitable dataset (e.g., including source code measures and corresponding defect\ndata) to build a simple defect prediction model and use it to predict the likelihood of defects using\nsource code measures from similar code.\nThe model should use at least four features from the dataset, and the class should explore the results\nusing several different features to highlight how the results change based on the selected features.\n11.",
  "3 Dataset Quality Issues ": "3 Dataset Quality Issues ........................................................................................................ 35\n4.",
  "4 Data Quality and its Effect on the ML Model ": "4 Data Quality and its Effect on the ML Model ...................................................................... 37\n4.",
  "5 Data Labelling for Supervised Learning ": "5 Data Labelling for Supervised Learning ............................................................................. 37\n4.5.",
  "1 Approaches to Data Labelling ": "1 Approaches to Data Labelling ........................................................................................ 37\n4.5.",
  "2 Mislabeled Data in Datasets ": "2 Mislabeled Data in Datasets .......................................................................................... 38\n",
  "5 ML Functional Performance Metrics ": "5 ML Functional Performance Metrics \u2013 120 minutes\nKeywords\nNone\nAI-Specific Keywords\nAccuracy, area under curve (AUC), confusion matrix, F1-score, inter-cluster metrics, intra-cluster\nmetrics, mean square error (MSE), ML benchmark suites, ML functional performance metrics,\nprecision, recall, receiver operating characteristic (ROC) curve, regression model, R-squared,\nsilhouette coefficient\nLearning Objectives for Chapter 5:\n5.",
  "1 Confusion Matrix ": "1 Confusion Matrix ................................................................................................................. 40\n5.",
  "2 Additional ML Functional Performance Metrics for Classification": "2 Additional ML Functional Performance Metrics for Classification,\nRegression and Clustering\nThere are numerous metrics for different types of ML problems (in addition to those related to\nclassification described in Section 5.1). Some of the most commonly used metrics are described\nbelow.\nSupervised Classification Metrics\n\u2022 The receiver operating characteristic (ROC) curve is a graphical plot that illustrates the ability\nof a binary classifier as its discrimination threshold is varied. The method was originally\ndeveloped for military radars, which is why it is so named. The ROC curve is plotted with true\npositive rate (TPR) (also known as recall) against the false positive rate (FPR = FP / (TN +\nFP)), with TPR on the y axis and FPR on the x axis.\n\u2022 The area under curve (AUC) is the area under the ROC curve. It represents the degree of\nseparability of a classifier, showing how well the model distinguishes between classes. With\na higher AUC, the model\u2019s predictions are better.\nSupervised Regression Metrics\nFor supervised regression models, the metrics represent how well the regression line fits the actual\ndata points.\n\u2022 Mean Square Error (MSE) is the average of the squared differences between the actual value\nand the predicted value. The value of MSE is always positive, and a value closer to zero\nsuggests a better regression model. By squaring the difference, it ensures positive and\nnegative errors do not cancel each other out.\n\u2022 R-squared (also known as the coefficient of determination) is a measure of how well the\nregression model fits the dependent variables.\nUnsupervised Clustering Metrics\nFor unsupervised clustering, there are several metrics that represent the distances between the\nvarious clusters and the closeness of data points within a given cluster.\n\u2022 Intra-cluster metrics measure the similarity of data points within a cluster.\n\u2022 Inter-cluster metrics measure the similarity of data points in different clusters.\n\u2022 The silhouette coefficient (also known as silhouette score) is a measure (between -1 and +1)\nbased on the average inter-cluster and intra-cluster distances. A score of +1 means the\nclusters are well-separated, a score of zero implies random clustering, and a score of -1\nmeans the clusters are wrongly assigned.\n5.",
  "3 Limitations of ML Functional Performance Metrics ": "3 Limitations of ML Functional Performance Metrics ............................................................. 41\n5.",
  "4 Selecting ML Functional Performance Metrics ": "4 Selecting ML Functional Performance Metrics ................................................................... 42\n5.4.",
  "5 Benchmark Suites for ML ": "5 Benchmark Suites for ML ................................................................................................... 43\n",
  "6 ML ": "6 ML - Neural Networks and Testing \u2013 65 minutes\nKeywords\nNone\nAI-Specific Keywords\nActivation value, deep neural network (DNN), ML training data, multi-layer perceptron, neural network,\nneuron coverage, perceptron, sign change coverage, sign-sign coverage, supervised learning,\nthreshold coverage, training data, value change coverage\nLearning Objectives for Chapter 6:\n6.",
  "1 Neural Networks ": "1 Neural Networks ................................................................................................................. 45\n6.1.",
  "2 Coverage Measures for Neural Networks ": "2 Coverage Measures for Neural Networks .......................................................................... 47\n",
  "7 Testing AI": "7 Testing AI-Based Systems Overview \u2013 115 minutes\nKeywords\nInput data testing, ML model testing\nAI-Specific Keywords\nAI component, automation bias, big data, concept drift, data pipeline, ML functional performance\nmetrics, training data\nLearning Objectives for Chapter 7:\n7.",
  "1 Specification of AI": "1 Specification of AI-Based Systems\nSystem requirements and design specifications are equally important for both AI-based systems and\nconventional systems. These specifications provide the basis for testers to check whether actual\nsystem behavior aligns with the specified requirements. However, if the specifications are incomplete\nand lack testability, this introduces a test oracle problem (see Section 8.7).\nThere are several reasons why the specification of AI-based systems can be particularly challenging:\n\u2022 In many AI-based systems projects, requirements are specified only in terms of high-level\nbusiness goals and required predictions. A reason for this is the exploratory nature of AI-\nbased system development. Often, AI-based systems projects start with a dataset, and the\ngoal is to determine which predictions can be obtained from that data. This is in contrast with\nspecifying the required logic from the start of a conventional project.\n\u2022 The accuracy of the AI-based system is often unknown until the test results from independent\ntesting are available. Along with the exploratory development approach, this often leads to\ninadequate specifications as implementation is already in progress by the time the desired\nacceptance criteria are determined.\n\u2022 The probabilistic nature of many AI-based systems can make it necessary to specify\ntolerances for some of the expected quality requirements, such as the accuracy of\npredictions.\n\u2022 Where the system goals call for replicating human behavior, rather than providing specific\nfunctionality, this often leads to poorly specified behavior requirements based on the system\nbeing as good as, or better than the human activities it aims to replace. This can make it\ndifficult to define a test oracle, especially when the humans it is replacing vary widely in their\ncapabilities.\n\u2022 Where AI is used to implement user interfaces, such as by natural language recognition,\ncomputer vision, or physical interaction with humans, the systems need to demonstrate\nincreased flexibility. However, such flexibility can also create challenges in identifying and\ndocumenting all the different ways in which such interactions might happen.\n\u2022 Quality characteristics specific to AI-based systems, such as adaptability, flexibility, evolution,\nand autonomy, need to be considered and defined as part of requirements specification (see\nChapter 2). The novelty of these characteristics can make them difficult to define and test.\n7.",
  "2 Test Levels for AI": "2 Test Levels for AI-Based Systems\nAI-based systems typically comprise both AI and non-AI components. Non-AI components can be\ntested using conventional approaches [I01], while AI components and systems containing AI\ncomponents may need to be tested differently in some respects, as described below. For all test\nlevels that include the testing of AI components, it is important for the testing to be closely supported\nby data engineers/scientists and domain experts.\nA major difference from the test levels used for conventional software is the inclusion of two new\nspecialized test levels to explicitly handle the testing of the input data and the models used in AI-\nbased systems [B15]. Most of this section is applicable to all AI-based systems, although some parts\nare specifically focused on ML.\nv1.",
  "1 Input Data Testing ": "1 Input Data Testing .......................................................................................................... 51\n7.2.",
  "2 ML Model Testing ": "2 ML Model Testing ........................................................................................................... 51\n7.2.",
  "3 Component Testing ": "3 Component Testing ........................................................................................................ 51\n7.2.",
  "4 Component Integration Testing ": "4 Component Integration Testing ...................................................................................... 51\n7.2.",
  "5 System Testing ": "5 System Testing ............................................................................................................... 51\n7.2.",
  "6 Acceptance Testing ": "6 Acceptance Testing ........................................................................................................ 52\n7.",
  "3 Test Data for Testing AI": "3 Test Data for Testing AI-based Systems\nDepending on the situation and the system under test (SUT), the acquisition of test data might\npresent a challenge. There are several potential challenges in dealing with test data for AI-based\nsystems, including:\n\u2022 Big data (high-volume, high-velocity and high-variety data) can be difficult to create and\nmanage. For example, it may be difficult to create representative test data for a system that\nconsumes large volumes of images and audio at a high speed.\n\u2022 Input data may need to change over time, particularly if it represents events in the real world.\nFor example, recorded photographs to test a facial recognition system may need to be \u201caged\u201d\nto represent the ageing of people over several years in real life.\n\u2022 Personal or otherwise confidential data may need special techniques for sanitization,\nencryption, or redaction. Legal approval for use may also be required.\n\u2022 When testers use the same implementation as the data scientists for data acquisition and\ndata pre-processing, then defects in these steps may be masked.\n7.",
  "4 Testing for Automation Bias in AI": "4 Testing for Automation Bias in AI-Based Systems\nOne category of AI-based systems helps humans in decision-making. However, there is occasionally\na tendency for humans to be too trusting of these systems. This misplaced trust may be called either\nautomation bias or complacency bias, and takes two forms.\n\u2022 The first form of automation/complacency bias is when the human accepts recommendations\nprovided by the system and fails to consider inputs from other sources (including\nthemselves). For example, a procedure where a human keys data into a form might be\nimproved by using machine learning to pre-populate the form, and the human then validates\nthis data. It has been shown that this form of automation bias typically reduces the quality of\nv1.",
  "5 Documenting an AI Component ": "5 Documenting an AI Component ......................................................................................... 53\n7.",
  "6 Testing for Concept Drift ": "6 Testing for Concept Drift ..................................................................................................... 54\n7.",
  "7 Selecting a Test Approach for an ML System ": "7 Selecting a Test Approach for an ML System .................................................................... 54\n",
  "8 Testing AI": "8 Testing AI-Specific Quality Characteristics \u2013 150\nminutes\nKeywords\nTest oracle\nAI-Specific Keywords\nAlgorithmic bias, autonomous system, autonomy, expert system, explainability, inappropriate bias,\ninterpretability, LIME method, ML training data, non-deterministic system, probabilistic system,\nsample bias, self-learning system, transparency\nLearning Objectives for Chapter 8:\n8.",
  "1 Challenges Testing Self": "1 Challenges Testing Self-Learning Systems\nThere are several potential challenges to overcome when testing self-learning systems (see Chapter\n2 for more details on these systems), including:\n\u2022 Unexpected change: The original requirements and constraints within which the system\nshould work are generally known, but there may be little or no information available on the\nchanges made by the system itself. It is normally possible to test against the original\nrequirements and design (and any specified constraints), but if the system has devised an\ninnovative implementation or gamed a solution (the implementation of which cannot be seen),\nit may be difficult to design tests which are appropriate for this new implementation. In\naddition, when systems change themselves (and their outputs), the results of previously\npassing tests can change. This is a test design challenge. It may be addressed by designing\nappropriate tests that remain relevant as the system changes its behavior, so preventing a\npotential regression testing problem. However, it may also require new tests to be designed\nbased on observed new system behaviors.\n\u2022 Complex acceptance criteria: It may be necessary to define expectations for improvement by\nthe system when it self-learns. For example, it may be assumed that if the system changes\nitself, its overall functional performance should improve. Additionally, specifying anything\nother than simple \u201cimprovement\u201d can quickly become complex. For example, a minimum\nimprovement might be expected (rather than simply any improvement), or the required\nimprovement may be linked to environmental factors (e.g., a minimum 10% improvement in\nfunctionality X is required if environmental factor F changes by more than Y). These\nproblems may be addressed through the specification and testing against the more complex\nacceptance criteria, and by maintaining a continuous record of the current system baseline\nfunctional performance.\n\u2022 Insufficient testing time: It may be necessary to know how quickly the system is expected to\nlearn and adapt, given different scenarios. These acceptance criteria may be difficult to\nspecify and acquire. If a system adapts quickly, there might be insufficient time to manually\nexecute new tests after each change, so it may be necessary to write tests that can be run\nautomatically when the system changes itself. These challenges can be addressed through\nthe specification of appropriate acceptance criteria (see Section 8.8) and automated\ncontinuous testing.\n\u2022 Resource requirements: The system requirements might include acceptance criteria for the\nresources which the system is permitted to use when performing self-learning or adaptation.\nThis may include, for example, the amount of processing time and memory allowed to be\nused to improve. Additionally, consideration needs to be given on whether this resource\nusage should be linked to a measurable improvement in functionality or accuracy. This\nchallenge affects the specification of acceptance criteria.\n\u2022 Insufficient specifications of operational environment: A self-learning system may change if\nthe environmental inputs that it receives are outside expected ranges, or if they are not\nreflected in the training data. These inputs may be attacks in the form of data poisoning (see\nSection 9.1.2). It can be difficult to predict the full range of operational environments and\nenvironmental changes, and to therefore identify the full set of representative test cases and\nenvironmental requirements. Ideally, the full scope of possible changes in the operational\nenvironment to which the system is expected to respond will be defined as acceptance\ncriteria.\n\u2022 Complex test environment: Managing the test environment to ensure it can mimic all the\npotential high-risk operational environment changes is a challenge and may involve the use of\nv1.",
  "2 Testing Autonomous AI": "2 Testing Autonomous AI-Based Systems\nAutonomous systems must be able to determine when they require human intervention and when\nthey do not. Therefore, testing the autonomy of AI-based systems requires that conditions are\ncreated for the system to exercise this decision-making.\nTesting for autonomy may require:\n\u2022 Testing whether the system requests human intervention for a specific scenario when the\nsystem should be relinquishing control. Such scenarios could include a change to the\noperational environment, or the system exceeding the limits of its autonomy.\n\u2022 Testing whether the system requests human intervention when the system should be\nrelinquishing control after a specified period of time.\n\u2022 Testing whether the system unnecessarily requests human intervention when it should still be\nworking autonomously.\nIt may be helpful to use boundary value analysis applied to the operating environment to generate the\nnecessary conditions for this testing. It can be challenging to define how the parameters that\ndetermine autonomy manifest themselves in the operating environment, and to create the test\nscenarios which depend on the nature of the autonomy.\n8.",
  "3 Testing for Algorithmic": "3 Testing for Algorithmic, Sample and Inappropriate Bias\nAn ML system should be evaluated against the different biases and actions taken to remove\ninappropriate bias. This may involve positive bias being deliberately introduced to counter the\ninappropriate bias.\nTesting with an independent dataset can often detect bias. However, it can be difficult to identify all\nthe data that causes bias because the ML algorithm can use combinations of seemingly unrelated\nfeatures to create unwanted bias.\nAI-based systems should be tested for algorithmic bias, sample bias and inappropriate bias (see\nSection 2.4). This may involve:\n\u2022 Analysis during the model\u2019s training, evaluation and tuning activities to identify whether\nalgorithmic bias is present.\n\u2022 Reviewing the source of the training data and the processes used to acquire it, such that the\npresence of sample bias can be identified.\n\u2022 Reviewing the pre-processing of data as part of the ML workflow to identify whether the data\nhas been affected in a way that could cause sample bias.\nv1.",
  "4 Challenges Testing Probabilistic and Non": "4 Challenges Testing Probabilistic and Non-Deterministic AI-Based\nSystems\nMost probabilistic systems are also non-deterministic, and so the following list of testing challenges\ntypically applies to AI-based systems with any of these attributes:\n\u2022 There may be multiple, valid outcomes from a test with the same set of preconditions and\ninputs. This makes the definition of expected results more challenging and can cause\ndifficulties:\no when tests are reused for confirmation testing.\no when tests are reused for regression testing.\no where reproducibility of testing is important.\no when the tests are automated.\n\u2022 The tester typically requires a deeper knowledge of the required system behavior so that they\ncan come up with reasonable checks for whether the test has passed rather than simply\nstating an exact value for the expected test result. For example, testers may need to define\nmore sophisticated expected results compared with conventional systems. These expected\ntest results may include tolerances (e.g., \u201cis the actual result within 2% of the optimal\nsolution?\u201d).\n\u2022 Where a single definitive output from a test is not possible due to the probabilistic nature of\nthe system, it is often necessary for the tester to run a test several times in order to generate\na statistically valid test result.\n8.",
  "5 Challenges Testing Complex AI": "5 Challenges Testing Complex AI-Based Systems\nAI-based systems are often used to implement tasks that are too complex for humans to perform.\nThis can lead to a test oracle problem because testers are unable to determine the expected results\nas they would normally do (see Section 8.7). For example, AI-based systems are often used to\nidentify patterns in large volumes of data. Such systems are used because they can find patterns that\nhumans, even after much analysis, simply cannot find manually. Understanding the required behavior\nof such systems in sufficient depth to be able to generate expected results can be challenging.\nA similar problem arises when the internal structure of an AI-based system is generated by software,\nmaking it too complex for humans to understand. This leads to the situation where the AI-based\nv1.",
  "6 Testing the Transparency": "6 Testing the Transparency, Interpretability and Explainability of AI-\nBased Systems\nInformation on how the system has been implemented may be provided by the system developers.\nThis may include the sources of training data, how labelling was conducted, and how the system\ncomponents have been designed. When this information is not available, it can make the design of\ntests challenging. For example, if training data information is not available, then identifying potential\ngaps in such data and testing the impact of those gaps, becomes difficult. This situation can be\ncompared to black-box and white-box testing, and has similar advantages and disadvantages.\nTransparency can be tested by comparing the information documented on the data and algorithm to\nthe actual implementation and determining how closely they match.\nWith ML, it can often be more difficult to explain the link between a specific input and a specific\noutput, than with conventional systems. This low level of explainability is primarily because the model\ngenerating the output is itself generated by code (the algorithm) and does not reflect the way humans\nthink about a problem. Different ML models provide different levels of explainability and should be\nselected based on the requirements for the system, which may include explainability and testability.\nOne method to understand explainability is through the dynamic testing of the ML model when\napplying perturbations to the test data. Methods exist for quantifying explainability in this manner and\nfor providing visual explanations of it. Some of these methods are model-agnostic, while others are\nspecific to a particular type of model and require access to it. Exploratory testing can also be used to\nbetter understand the relationship between the inputs and outputs of a model.\nThe LIME method is model-agnostic and uses dynamically injected input perturbations and the\nanalysis of outputs to provide testers with a view of the relationship between inputs and outputs. This\ncan be an effective method for providing model explainability. However it is limited to providing\npossible reasons for the outputs, rather than a definitive reason, and is not applicable for all types of\nalgorithms.\nThe interpretability of an AI-based system is heavily dependent on who this applies to. Different\nstakeholders may have different requirements in terms of how well they need to grasp the underlying\ntechnology.\nMeasuring and testing the level of understanding for both interpretability and explainability can be\nchallenging as stakeholders will vary in their levels of ability and may not agree. In addition,\nidentifying the profile of typical stakeholders may be difficult for many types of systems. Where\nperformed, this testing typically takes the form of user surveys and/or questionnaires.\nv1.",
  "7 Test Oracles for AI": "7 Test Oracles for AI-Based Systems\nA major problem with the testing of AI-based systems can be the specification of expected results. A\ntest oracle is the source used to determine the expected result of a test [I01]. A challenge in\ndetermining expected results is known as the test oracle problem.\nWith complex, non-deterministic or probabilistic systems, it can be difficult to establish a test oracle\nwithout knowing the \u201cground truth\u201d (i.e., the actual result in the real world that the AI-based system is\ntrying to predict). This \u201cground truth\u201d is distinct from a test oracle, in that a test oracle may not\nnecessarily provide an expected value, but only a mechanism with which to determine whether the\nsystem is operating correctly or not.\nAI-based systems can evolve (see Section 2.3), and the testing of self-learning systems (see Section\n8.1) can also suffer from test oracle problems as they modify themselves and can thereby make it\nnecessary to frequently update the functional expectations of the system.\nA further cause of difficulty in obtaining an effective test oracle is that in many cases, the correctness\nof the software behavior is subjective. Virtual assistants (e.g., Siri and Alexa) are an example of this\nproblem in that different user often have quite different expectations and may experience different\nresults depending on their choice of words and clarity of speech.\nIn some situations, it may be possible to define the expected result with limits or tolerances. For\nexample, the stopping point for an autonomous car could be defined as within a maximum distance of\na specific point. In the context of expert systems, the determination of the expected results may be\nachieved by consulting an expert (noting that the expert\u2019s opinion may still be wrong). There are\nseveral important factors to consider in such circumstances:\n\u2022 Human experts vary in their levels of competence. Experts involved need to be at least as\ncompetent as the experts the system is intended to replace.\n\u2022 Experts may not agree with each other, even when presented with the same information.\n\u2022 Human experts may not approve of the automation of their judgement. In such cases their\nrating of potential outputs should be double-blind (i.e., neither the experts nor the evaluators\nof the outputs should know which ratings were automated).\n\u2022 Humans are more likely to caveat responses (e.g., with phrases like \u201cI\u2019m not sure, but...\u201d). If\nthis kind of caveat is not available to the AI-based system, this should be considered when\ncomparing the responses.\nTest techniques exist which can alleviate the test oracle problem, such as A/B testing (see Section\n9.4), back-to-back testing (see Section 9.3) and metamorphic testing (see Section 9.5).\n8.",
  "8 Test Objectives and Acceptance Criteria ": "8 Test Objectives and Acceptance Criteria ........................................................................... 62\n",
  "9 Methods and Techniques for the Testing of AI": "9 Methods and Techniques for the Testing of AI-Based\nSystems \u2013 245 minutes\nKeywords\nA/B testing, adversarial testing, back-to-back testing, error guessing, experience-based testing,\nexploratory testing, metamorphic relation (MR), metamorphic testing (MT), pairwise testing, pseudo-\noracle, test oracle problem, tours\nAI-Specific Keywords\nAdversarial attack, adversarial example, data poisoning, ML system, trained model\nLearning Objectives for Chapter 9:\n9.",
  "1 Adversarial Attacks and Data Poisoning ": "1 Adversarial Attacks and Data Poisoning ............................................................................ 66\n9.1.",
  "1 Adversarial Attacks ": "1 Adversarial Attacks ......................................................................................................... 66\n9.1.",
  "2 Data Poisoning ": "2 Data Poisoning ............................................................................................................... 66\n9.",
  "2 Pairwise Testing ": "2 Pairwise Testing ................................................................................................................. 67\n9.2.",
  "3 Back": "3 Back-to-Back Testing\nOne of the potential solutions to the test oracle problem (see Section 8.7) when testing AI-based\nsystems is to use back-to-back testing. This is also known as differential testing. With back-to-back\ntesting, an alternative version of the system is used as a pseudo-oracle and its outputs compared with\nthe test results produced by the SUT. The pseudo-oracle could be an existing system, or it could be\ndeveloped by a different team, possibly on a different platform, with a different architecture and with a\nv1.",
  "5 Metamorphic Testing ": "5 Metamorphic Testing (MT)\nMetamorphic testing [B18] is a technique aimed at generating test cases which are based on a source\ntest case that has passed. One or more follow-up test cases are generated by changing\n(metamorphizing) the source test case based on a metamorphic relation (MR). The MR is based on a\nv1.",
  "6 Experience": "6 Experience-Based Testing of AI-Based Systems\nExperience-based testing includes error guessing, exploratory testing, and checklist-based testing\n[I01], all of which can be applied to the testing of AI-based systems.\nError guessing is typically based on testers knowledge, typical developer errors, and failures in similar\nsystems (or previous versions). An example of error guessing applied to AI-based systems could be\nthe use of knowledge about how ML systems have in the past failed due to the use of systemically\nbiased training data.\nIn exploratory testing, tests are designed, generated, and executed in an iterative manner, with the\nopportunity for later tests to be derived, based on the test results of earlier tests. Exploratory testing\nis especially useful when there are poor specifications or test oracle problems, which is often the case\nfor AI-based systems. As a result, exploratory testing is often used in this context and should be used\nto supplement the more systematic testing based on techniques, such as metamorphic testing (see\nSection 9.5).\nA tour is a metaphor used for a set of strategies and goals for testers to refer to when they perform\nexploratory testing organized around a special focus [B20]. Typical tours for the exploratory testing of\nAI-based systems might focus on the concepts of bias, underfitting and overfitting in ML systems. For\nexample, a data tour might be applied to test the model. In this tour the tester could identify different\ntypes of data used for training, their distribution, their variations, their format and ranges, etc., and\nthen use the data types to test the model.\nML systems are highly dependent on the quality of training data, and the existing field of EDA is\nclosely related to the exploratory testing approach. EDA is where data are examined for patterns,\nrelationships, trends and outliers. It involves the interactive, hypothesis-driven exploration of data and\nis described in [B21] as \u201cWe explore data with expectations. We revise our expectations based on\nwhat we see in the data. And we iterate this process.\u201d EDA typically requires tool support in two\nareas; for interaction with the data, to allow analysts to better understand complex data, and for data\nvisualization, to allow them to easily display analysis results. The use of exploratory techniques,\nprimarily driven by data visualization, can help validate the ML algorithm being used, identify changes\nthat result in efficient models, and leverage domain expertise [B22].\nGoogle has a set of twenty-eight ML tests written as assertions, in the areas of data, model\ndevelopment, infrastructure and monitoring, which is used as a testing checklist within Google for ML\nsystems [B23]. The Google \u201cML test checklist\u201d is presented here as published by Google:\nML Data:\n",
  "7 Selecting Test Techniques for AI": "7 Selecting Test Techniques for AI-Based Systems\nAn AI-based system will typically include both AI and non-AI components. The selection of test\ntechniques for testing the non-AI components is generally the same as for any conventional testing.\nFor the AI-based components, the choice may be more constrained. For example, where a test\noracle problem is perceived (i.e., generating expected results is difficult), then, based on the\nperceived risks, it is possible to mitigate this problem by the use of the following:\n\u2022 Back-to-back testing: This requires test cases to be available or generated and an equivalent\nsystem to act as a pseudo-oracle, which for regression testing can be a previous version of\nthe system. For effective detection of defects, an independently developed system may be\nrequired.\n\u2022 A/B testing: This often uses operational inputs as test cases and is normally used to compare\ntwo variants of the same system using statistical analysis. A/B testing can be used to check\nfor the data poisoning of a new variant, or for automated regression testing of a self-learning\nsystem.\n\u2022 Metamorphic testing: This can be used by inexperienced testers to cost-effectively find\ndefects although they need to understand the application domain. MT is not suitable for\nproviding definitive results as the expected results are not absolute, but, instead, relative to\nthe source test cases. Commercial tool support is not currently available, but many tests can\nbe generated manually.\nAdversarial testing is typically appropriate for ML models where the mishandling of adversarial\nexamples could have a significant impact, or where the system may be attacked. Similarly, testing for\ndata poisoning may be appropriate for ML systems where the system may be attacked.\nWhere the AI-based systems are complex and have multiple parameters, pairwise testing is often\nappropriate.\nExperience-based testing is often suitable for testing AI-based systems, especially for consideration\nof the data used for training and operational data. EDA can be used to validate the ML algorithm\nbeing used, identify efficiency improvements, and leverage domain expertise. Google have found that\ntheir ML test checklist is an effective approach for ML systems.\nIn the specific area of neural networks, coverage of the network is often suitable for mission-critical\nsystems, with some coverage criteria requiring more rigorous coverage than others.\nv1.",
  "10 Test Environments for AI": "10 Test Environments for AI-Based Systems \u2013 30 minutes\nKeywords\nVirtual test environment\nAI-Specific Keywords\nAI-specific processor, autonomous system, big data, explainability, multi-agent system, self-learning\nsystem\nLearning Objectives for Chapter 10:\n10.",
  "1 Test Environments for AI": "1 Test Environments for AI-Based Systems\nAI-based systems can be used in a wide variety of operational environments, which means that the\ntest environments are similarly diverse. Characteristics of AI-based systems that can cause the test\nenvironments to differ from those for conventional systems include:\n\u2022 Self-learning: Self-learning systems, and some autonomous systems, are expected to adapt\nto changing operational environments that may not have been fully defined when the system\nwas initially deployed (see Section 2.1). As a result, defining test environments that can\nmimic these undefined environmental changes is inherently difficult and may require both\nimagination on the part of the testers and a level of randomness built into the test\nenvironment.\n\u2022 Autonomy: Autonomous systems are expected to respond to changes in their environment\nwithout human intervention, and also recognize situations where autonomy should be ceded\nback to human operators (see Section 2.2). For some systems, identifying and then\nmimicking the circumstances for ceding autonomy may require the test environments to push\nthe systems to extremes. For some autonomous systems, their purpose is to work in\nenvironments that are hazardous, and setting up representative, hazardous test environments\ncan be challenging.\n\u2022 Multi-agency: Where multi-agent AI-based systems are expected to work in concert with other\nAI-based systems, the test environment may need to incorporate a level of non-determinism\nso that it can mimic the non-determinism of the AI-based systems with which the SUT\ninteracts.\n\u2022 Explainability: The nature of some AI-based systems can make it difficult to determine how\nthe system made its decisions (see Section 2.7). Where this is important to understand prior\nto deployment, the test environment may need to incorporate tools as a means of explaining\nhow decisions are made.\n\u2022 Hardware: Some of the hardware used to host AI-based systems is specifically designed for\nthis purpose, such as AI-specific processors (see Section 1.6). The need to include such\nhardware in the test environment should be considered as part of the relevant test planning.\n\u2022 Big data: Where an AI-based system is expected to consume big data (e.g., high-volume,\nhigh-velocity and/or high-variety data), then setting this up as part of a test environment\nneeds careful planning and implementation (see Section 7.3).\n10.",
  "2 Virtual Test Environments for Testing AI": "2 Virtual Test Environments for Testing AI-Based Systems\nThe use of a virtual test environment when testing an AI-based system brings the following benefits:\n\u2022 Dangerous scenarios: These can be tested without endangering the SUT, other interacting\nsystems, including humans, or the operational environment (e.g., trees, buildings).\n\u2022 Unusual scenarios: These can be tested when it would otherwise be very time consuming or\nexpensive to set up these scenarios for real operations (e.g., waiting for a rare event, such as\na full solar eclipse or four buses entering the same road intersection simultaneously).\nSimilarly, edge cases, which are difficult to create in the real world, can be created more\neasily, repeatedly and reproducibly in a virtual test environment.\n\u2022 Extreme scenarios: These can be tested when it would be expensive or impossible to set\nthese up in reality (e.g., for a nuclear disaster or deep space exploration).\nv1.",
  "11 Using AI for Testing ": "11 Using AI for Testing \u2013 195 minutes\nKeywords\nVisual testing\nAI-Specific Keywords\nBayesian techniques, classification, clustering algorithm, defect prediction, graphical user interface\n(GUI)\nLearning Objectives for Chapter 11:\n11.",
  "1 AI Technologies for Testing ": "1 AI Technologies for Testing ................................................................................................ 77\n11.1.",
  "2 Using AI to Analyze Reported Defects ": "2 Using AI to Analyze Reported Defects ............................................................................... 77\n11.",
  "3 Using AI for Test Case Generation ": "3 Using AI for Test Case Generation ..................................................................................... 78\n11.",
  "4 Using AI for the Optimization of Regression Test Suites ": "4 Using AI for the Optimization of Regression Test Suites .................................................... 78\n11.",
  "5 Using AI for Defect Prediction ": "5 Using AI for Defect Prediction ............................................................................................ 78\n11.5.",
  "6 Using AI for Testing User Interfaces ": "6 Using AI for Testing User Interfaces ................................................................................... 79\n11.6.",
  "1 Using AI to Test Through the Graphical User Interface ": "1 Using AI to Test Through the Graphical User Interface (GUI)\nTesting through the GUI is the typical approach for manual testing (other than for component testing)\nand is often the starting point for test automation initiatives. The resultant tests emulate human\ninteraction with the test object. This scripted test automation can be implemented by applying a\ncapture/playback approach, using either the actual coordinates of the user interface elements, or the\nsoftware-defined objects/widgets of the interface. However, this approach suffers several drawbacks\nwith object identification, including sensitivity to interface changes, code changes, and platform\nchanges.\nAI can be used to reduce the brittleness of this approach, by employing AI-based tools to identify the\ncorrect objects using various criteria (e.g., XPath, label, id, class, X/Y coordinates), and to choose the\nhistorically most stable identification criteria. For example, the ID of a button in a particular area of\nthe application may change with each release, and so the AI-based tool may assign a lower\nimportance to this ID over time and place more reliance on other criteria. This approach classifies the\nobjects in the user interface as matching the test, or not matching the test.\nAlternatively, visual testing uses image recognition to interact with GUI objects through the same\ninterface as an actual user, and therefore does not need to access the underlying code and interface\ndefinitions. This makes it completely non-intrusive and independent of the underlying technology. The\nscripts need only work through the visible user interface. This approach allows the tester to create\nscripts that interact directly with the images, buttons and text fields on the screen in the same way as\na human user, without being affected by the overall screen layout. The use of image recognition in\ntest automation can become restricted by the computing resources needed. However, the availability\nof affordable AI that supports sophisticated image recognition now makes this approach possible for\nmainstream use.\nv1.",
  "2 Using AI to Test the GUI ": "2 Using AI to Test the GUI ............................................................................................ 80\n",
  "12 References ": "12 References .................................................................................................................................. 81\n12.",
  "1 Standards ": "1 Standards [S] ...................................................................................................................... 81\n12.",
  "2 ISTQB": "2 ISTQB\u00ae Documents [I] ........................................................................................................ 81\n12.",
  "3 Books and Articles ": "3 Books and Articles [B] ........................................................................................................ 82\n12.",
  "4 Other References ": "4 Other References [R] .......................................................................................................... 85\n",
  "13 Appendix A ": "13 Appendix A \u2013 Abbreviations\nAbbreviation Description\nAI Artificial intelligence\nAIaaS AI as a service\nAPI Application programming interface\nAUC Area under curve\nDL Deep learning\nDNN Deep neural network\nEDA Exploratory data analysis\nEU European Union\nFN False negative\nFP False positive\nGDPR General data protection regulation\nGPU Graphical processing unit\nGUI Graphical user interface\nLIME Local interpretable model-agonistic explanations\nMC/DC Modified condition decision coverage\nML Machine learning\nMR Metamorphic relation\nMSE Mean square error\nMT Metamorphic testing\nNLP Natural language processing\nROC Receiver operating characteristic\nSUT System under test\nSVM Support vector machine\nTN True negative\nTP True positive\nXAI Explainable AI\nv1.",
  "14 Appendix B ": "14 Appendix B \u2013 AI Specific and other Terms\nTerm Name Definition\nThe ML functional performance metric used to evaluate a classifier,\naccuracy which measures the proportion of predictions that were correct\n(After ISO/IEC TR 29119-11)\nThe formula associated with a neuron in a neural network that\nactivation function\ndetermines the output of the neuron from the inputs to the neuron\nactivation value The output of an activation function of a neuron in a neural network\nThe deliberate use of adversarial examples to cause an ML model to\nadversarial attack\nfail\nA software licensing and delivery model in which AI and AI\nAI as a Service (AIaaS)\ndevelopment services are centrally hosted\nAI component A component that provides AI functionality\nThe situation when a previously labelled AI system is no longer\nAI effect\nconsidered to be AI as technology advances (ISO/IEC TR 29119-11)\nAI-based system A system that integrates one or more AI components\nAI-specific processor A type of specialized hardware designed to accelerate AI applications\nalgorithmic bias A type of bias caused by the ML algorithm\nThe activity of identifying objects in images with bounding boxes to\nannotation\nprovide labelled data for classification\nA measure of how well a classifier can distinguish between two\narea under curve (AUC)\nclasses.\nThe capability of an engineered system to acquire, process, create\nartificial intelligence (AI)\nand apply knowledge and skills (ISO/IEC TR 29119-11)\nAn unsupervised learning technique that identifies relationships and\nassociation\ndependencies between samples\naugmentation The activity of creating new data points based on an existing dataset\nautomation bias\nA type of bias caused by a person favoring the recommendations of an\nautomated decision-making system over other sources\nv1.",
  "15 Index ": "15 Index ........................................................................................................................................... 98\nv1.",
  "2021.\nIt was produced by a team from the International Software Testing Qualifications Board": "2021.\nIt was produced by a team from the International Software Testing Qualifications Board: Klaudia\nDussa-Zieger (chair), Werner Henschelchen, Vipul Kocher, Qin Liu, Stuart Reid, Kyle Siemens, and\nAdam Leon Smith.\nThe team thanks the authors of the three contributing syllabi;\n\u2022\nA4Q: Rex Black, Bruno Legeard, Jeremias R\u00f6\u00dfler, Adam Leon Smith, Stephan Goericke,\nWerner Henschelchen\n\u2022\nAiU: Main authors Vipul Kocher, Saurabh Bansal, Srinivas Padmanabhuni and Sonika\nBengani\nand co-authors Rik Marselis, Jos\u00e9 M. Diaz Delgado\n\u2022\nKSTQB & CSTQB AIT: Stuart Reid, Qin Liu, Yeoungjae Choi, Murian Song, Wonil Kwon\nThe team thanks the Exam, Glossary and Marketing Working Groups for their support throughout the\ndevelopment of the syllabus, Graham Bath for his technical editing and the Member Boards for their\nsuggestions and input.\nThe following persons participated in the reviewing and commenting of this syllabus:\nLaura Albert, Reto Armuzzi, \u00c1rp\u00e1d Besz\u00e9des, Armin Born, G\u00e9za Bujdos\u00f3, Renzo Cerquozzi, Sudeep\nChatterjee, Seunghee Choi, Young-jae Choi, Piet de Roo, Myriam Christener, Jean-Baptiste\nCrouigneau, Guofu Ding,Erwin Engelsma, Hongfei Fan, P\u00e9ter F\u00f6ldh\u00e1zi Jr., Tam\u00e1s Gergely,\nFerdinand Gramsamer, Attila Gy\u00fari, Matthias Hamburg, Tobias Horn, Jaros\u0142aw Hryszko, Beata\nKarpinska, Joan Killeen, Rik Kochuyt, Thomas Letzkus, Chunhui Li, Haiying Liu, Gary Mogyorodi, Rik\nMarselis, Imre M\u00e9sz\u00e1ros, Tetsu Nagata, Ingvar Nordstr\u00f6m, G\u00e1bor P\u00e9terffy, Tal Pe'er, Ralph Pichler,\nNishan Portoyan, Meile Posthuma, Adam Roman, Gerhard Runze, Andrew Rutz, Klaus Skafte, Mike\nSmith, Payal Sobti, P\u00e9ter S\u00f3t\u00e9r, Michael Stahl, Chris van Bael, Stephanie van Dijck, Robert\nWerkhoven, Paul Weymouth, Dong Xin, Ester Zabar, Claude Zhang.\nv1.",
  "0 Introduction\n": "0 Introduction\n0.",
  "1 Purpose of this Syllabus\nThis syllabus forms the basis for the ISTQB": "1 Purpose of this Syllabus\nThis syllabus forms the basis for the ISTQB\u00ae Certified Tester AI Testing. The ISTQB\u00ae provides this\nsyllabus as follows:\n",
  "1. To member boards": "1. To member boards, to translate into their local language and to accredit training providers.\nMember boards may adapt the syllabus to their particular language needs and modify the\nreferences to adapt to their local publications.\n",
  "2. To certification bodies": "2. To certification bodies, to derive examination questions in their local language adapted to the\nlearning objectives for this syllabus.\n",
  "3. To training providers": "3. To training providers, to produce courseware and determine appropriate teaching methods.\n",
  "4. To certification candidates": "4. To certification candidates, to prepare for the certification exam (either as part of a training\ncourse or independently).\n",
  "5. To the international software and systems engineering community": "5. To the international software and systems engineering community, to advance the profession\nof software and systems testing, and as a basis for books and articles.\n0.",
  "2 The Certified Tester AI Testing\nThe Certified Tester AI Testing is aimed at anyone involved in testing AI": "2 The Certified Tester AI Testing\nThe Certified Tester AI Testing is aimed at anyone involved in testing AI-based systems and/or AI for\ntesting. This includes people in roles such as testers, test analysts, data analysts, test engineers, test\nconsultants, test managers, user acceptance testers and software developers. This certification is\nalso appropriate for anyone who wants a basic understanding of testing AI-based systems and/or AI\nfor testing, such as project managers, quality managers, software development managers, business\nanalysts, operations team members, IT directors and management consultants.\nThe Certified Tester AI Testing Overview [I03] is a separate document which includes the following\ninformation:\n\u2022 Business outcomes for the syllabus\n\u2022 Matrix of business outcomes and connection with learning objectives\n\u2022 Summary of the syllabus\n\u2022 Relationships among the syllabi\n0.",
  "3 Examinable Learning Objectives and Cognitive Level of\nKnowledge\nLearning objectives support the business outcomes and are used to create the Certified Tester AI\nTesting exams": "3 Examinable Learning Objectives and Cognitive Level of\nKnowledge\nLearning objectives support the business outcomes and are used to create the Certified Tester AI\nTesting exams.\nCandidates may be asked to recognize, remember, or recall a keyword or concept mentioned in any\nof the eleven chapters. The specific learning objectives levels are shown at the beginning of each\nchapter, and classified as follows:\n\u2022 K1: Remember\n\u2022 K2: Understand\nv1.",
  "5 The Certified Tester AI Testing Exam\nThe Certified Tester AI Testing exam will be based on this syllabus": "5 The Certified Tester AI Testing Exam\nThe Certified Tester AI Testing exam will be based on this syllabus. Answers to exam questions may\nrequire the use of material based on more than one section of this syllabus. All sections of the\nsyllabus are examinable, except for the Introduction and Appendices. Standards and books are\nincluded as references, but their content is not examinable, beyond what is summarized in the\nsyllabus itself from such standards and books.\nRefer to Certified Tester Specialist AI Testing \u201cOverview\u201d document for further details under section\n\u201cExam Structure\u201d.\nEntry Requirement Note: The ISTQB\u00ae Foundation Level certificate shall be obtained before taking the\nCertified Tester Specialist AI Testing exam.\nv1.",
  "6 Accreditation\nAn ISTQB": "6 Accreditation\nAn ISTQB\u00ae Member Board may accredit training providers whose course material follows this\nsyllabus. Training providers should obtain accreditation guidelines from the Member Board or body\nthat performs the accreditation. An accredited course is recognized as conforming to this syllabus\nand is allowed to have an ISTQB\u00ae exam as part of the course.\nThe accreditation guidelines for this syllabus follow the general Accreditation Guidelines published by\nthe Processes Management and Compliance Working Group.\n0.",
  "7 Level of Detail\nThe level of detail in this syllabus allows internationally consistent courses and exams": "7 Level of Detail\nThe level of detail in this syllabus allows internationally consistent courses and exams. In order to\nachieve this goal, the syllabus consists of:\n\u2022 General instructional objectives describing the intention of the AI Testing Certified Tester.\n\u2022 A list of terms that students must be able to recall.\n\u2022 Learning and hands-on objectives for each knowledge area, describing the learning outcomes\nto be achieved.\n\u2022 A description of the key concepts, including references to sources such as accepted literature\nor standards.\nThe syllabus content is not a description of the entire knowledge area for the testing of AI-based\nsystems; it reflects the level of detail to be covered in Certified Tester Specialist AI Testing training\ncourses. It focuses on introducing the basic concepts of artificial intelligence (AI) and machine\nlearning in particular, and how systems based on these technologies can be tested.\n0.",
  "8 How this Syllabus is Organized\nThere are eleven chapters with examinable content": "8 How this Syllabus is Organized\nThere are eleven chapters with examinable content. The top-level heading for each chapter specifies\nthe time for the chapter; timing is not provided below chapter level. For accredited training courses,\nthe syllabus requires a minimum of 25.1 hours of instruction, distributed across the eleven chapters\nas follows:\n\u2022 Chapter 1: 105 minutes Introduction to AI\n\u2022 Chapter 2: 105 minutes Quality Characteristics for AI-Based Systems\n\u2022 Chapter 3: 145 minutes Machine Learning (ML) \u2013 Overview\n\u2022 Chapter 4: 230 minutes ML \u2013 Data\n\u2022 Chapter 5: 120 minutes ML Functional Performance Metrics\n\u2022 Chapter 6: 65 minutes ML \u2013 Neural Networks and Testing\n\u2022 Chapter 7: 115 minutes Testing AI-Based Systems Overview\n\u2022 Chapter 8: 150 minutes Testing AI-Specific Quality Characteristics\n\u2022 Chapter 9: 245 minutes Methods and Techniques for the Testing of AI-Based Systems\n\u2022 Chapter 10: 30 minutes Test Environments for AI-Based Systems\n\u2022 Chapter 11: 195 minutes Using AI for Testing\nv1.",
  "1 Definition of AI and AI Effect\nAI": "1 Definition of AI and AI Effect\nAI-1.1.1 K",
  "2 Describe the AI effect and how it influences the definition of AI": "2 Describe the AI effect and how it influences the definition of AI.\n1.",
  "2 Distinguish between narrow AI": "2 Distinguish between narrow AI, general AI, and super AI.\n1.",
  "2 Differentiate between AI": "2 Differentiate between AI-based systems and conventional systems.\n1.",
  "4 AI Technologies\nAI": "4 AI Technologies\nAI-1.4.1 K",
  "1 Recognize the different technologies used to implement AI": "1 Recognize the different technologies used to implement AI.\n1.",
  "5 AI Development Frameworks\nAI": "5 AI Development Frameworks\nAI-1.5.1 K",
  "1 Identify popular AI development frameworks": "1 Identify popular AI development frameworks.\n1.",
  "2 Compare the choices available for hardware to implement AI": "2 Compare the choices available for hardware to implement AI-based systems.\n1.",
  "2 Explain the concept of AI as a Service ": "2 Explain the concept of AI as a Service (AIaaS).\n1.",
  "2 Explain the use of pre": "2 Explain the use of pre-trained AI models and the risks associated with them.\n1.",
  "2 Describe how standards apply to AI": "2 Describe how standards apply to AI-based systems.\nv1.",
  "1 Definition of AI and AI Effect\nThe term artificial intelligence ": "1 Definition of AI and AI Effect\nThe term artificial intelligence (AI) dates back to the 1950s and refers to the objective of building and\nprogramming \u201cintelligent\u201d machines capable of imitating human beings. The definition today has\nevolved significantly, and the following definition captures the concept [S01]:\nThe capability of an engineered system to acquire, process and apply knowledge and skills.\nThe way in which people understand the meaning of AI depends on their current perception. In the\n1970s the idea of a computer system that could beat a human at chess was somewhere in the future\nand most considered this to be AI. Now, over twenty years after the computer-based system Deep\nBlue beat world chess champion Garry Kasparov, the \u201cbrute force\u201d approach implemented in that\nsystem is not considered by many to be true artificial intelligence (i.e., the system did not learn from\ndata and was not capable of self-learning). Similarly, the expert systems of the 1970s and 1980s\nincorporated human expertise as rules which could be run repeatedly without the expert being\npresent. These were considered to be AI then, but are not considered as such now.\nThe changing perception of what constitutes AI is known as the \u201cAI Effect\u201d [R01]. As the perception of\nAI in society changes, so does its definition. As a result, any definition made today is likely to change\nin the future and may not match those from the past.\n1.",
  "4 AI Technologies\nAI can be implemented using a wide range of technologies ": "4 AI Technologies\nAI can be implemented using a wide range of technologies (see [B02] for more details), such as:\n\u2022 Fuzzy logic\n\u2022 Search algorithms\n\u2022 Reasoning techniques\n- Rule engines\n- Deductive classifiers\n- Case-based reasoning\n- Procedural reasoning\n\u2022 Machine learning techniques\n- Neural networks\n- Bayesian models\n- Decision trees\n- Random forest\n- Linear regression\n- Logistic regression\n- Clustering algorithms\n- Genetic algorithms\n- Support vector machine (SVM)\nAI-based systems typically implement one or more of these technologies.\n1.",
  "5 AI Development Frameworks\nThere are many AI development frameworks available": "5 AI Development Frameworks\nThere are many AI development frameworks available, some of which are focused on specific\ndomains. These frameworks support a range of activities, such as data preparation, algorithm\nselection, and compilation of models to run on various processors, such as central processing units\n(CPUs), graphical processing units (GPUs) or Cloud Tensor Processing Units (TPUs). The selection\nof a particular framework may also depend on particular aspects such as the programming language\nused for the implementation and its ease of use. The following frameworks are some of the most\npopular (as of April 2021):\n\u2022 Apache MxNet: A deep learning open-source framework used by Amazon for Amazon Web\nServices (AWS) [R02].\n\u2022 CNTK: The Microsoft Cognitive Toolkit (CNTK) is an open-source deep-learning toolkit [R03].\n\u2022 IBM Watson Studio: A suite of tools that support the development of AI solutions [R04].\nv1.",
  "1 Contracts for AI as a Service\nThese AI services are typically provided with similar contracts as for non": "1 Contracts for AI as a Service\nThese AI services are typically provided with similar contracts as for non-AI cloud-based Software as\na Service (SaaS). A contract for AIaaS typically includes a service-level agreement (SLA) that\ndefines availability and security commitments. Such SLAs typically cover an uptime for the service\n(e.g., 99.99% uptime) and a response time to fix defects, but rarely define ML functional performance\nmetrics, (such as accuracy), in a similar manner (see Chapter 5). AIaaS is often paid for on a\nsubscription basis, and if the contracted availability and/or response times are not met, then the\nservice provider typically provides credits for future services. Other than these credits, most AIaaS\ncontracts provide limited liability (other than in terms of fees paid), meaning that AI-based systems\nthat depend on AIaaS are typically limited to relatively low-risk applications, where loss of service\nwould not be too damaging.\nServices often come with an initial free trial period in lieu of an acceptance period. During this period\nthe consumer of the AIaaS is expected to test whether the provided service meets their needs in\nterms of required functionality and performance (e.g., accuracy). This is generally necessary to cover\nany lack of transparency on the provided service (see Section 7.5).\n1.7.",
  "2 AIaaS Examples\nThe following are examples of AIaaS ": "2 AIaaS Examples\nThe following are examples of AIaaS (as of April 2021):\n\u2022 IBM Watson Assistant: This is an AI chatbot which is priced according to the number of\nmonthly active users.\nv1.",
  "2 Transfer Learning\nIt is also possible to take a pre": "2 Transfer Learning\nIt is also possible to take a pre-trained model and modify it to perform a second, different requirement.\nThis is known as transfer learning and is used on deep neural networks in which the early layers (see\nChapter 6) of the neural network typically perform quite basic tasks (e.g., identifying the difference\nbetween straight and curved lines in an image classifier), whereas the later layers perform more\nspecialized tasks (e.g., differentiating between building architectural types). In this example, all but the\nlater layers of an image classifier can be reused, eliminating the need to train the early layers. The\nlater layers are then retrained to handle the unique requirements for a new classifier. In practice, the\npre-trained model may be fine-tuned with additional training on new problem-specific data.\nThe effectiveness of this approach largely depends on the similarity between the function performed\nby the original model and the function required by the new model. For example, modifying an image\nclassifier that identifies cat species to then identify dog breeds would be far more effective than\nmodifying it to identify people\u2019s accents.\nThere are many pre-trained models available, especially from academic researchers. Some\nexamples of such pre-trained models are ImageNet models [R14] such as Inception, VGG, AlexNet,\nand MobileNet for image classification and pre-trained NLP models like Google\u2019s BERT [R15].\nv1.",
  "2017. In addition": "2017. In addition, ISO/IEC JTC1/SC7, which covers software and system\nengineering, has published a technical report on the \u201cTesting of AI-based systems\u201d [S01].\nStandards on AI are also published at the regional level (e.g., European standards) and the national\nlevel.\nThe EU-wide General Data Protection Regulation (GDPR) came into effect in May 2018 and sets\nobligations for data controllers with regards to personal data and automated decision-making [B06]. It\nincludes requirements to assess and improve AI system functional performance, including the\nmitigation of potential discrimination, and for ensuring individuals\u2019 rights to not be subjected to\nautomated decision-making. The most important aspect of the GDPR from a testing perspective is\nthat personal data (including predictions) should be accurate. This does not mean that every single\nprediction made by the system must be accurate, but that the system should be accurate enough for\nthe purposes for which it is used.\nThe German national standards body (DIN) has also developed the AI Quality Metamodel ([S02],\n[S03]).\nStandards on AI are also published by industry bodies. For example, the Institute of Electrical and\nElectronics Engineers (IEEE) is working on a range of standards on ethics and AI (The IEEE Global\nInitiative for Ethical Considerations in Artificial Intelligence and Autonomous Systems). Many of these\nstandards are still in development at the time of writing.\nv1.",
  "26262. Standards in isolation are voluntary\ndocuments": "26262. Standards in isolation are voluntary\ndocuments, and their use is normally only made mandatory by legislation or contract. However, many\nusers of standards do so to benefit from the expertise of the authors and to create products that are of\nhigher quality.\nv1.",
  "1 Flexibility and Adaptability\nAI": "1 Flexibility and Adaptability\nAI-2.1.1 K",
  "2 Explain the importance of flexibility and adaptability as characteristics of AI": "2 Explain the importance of flexibility and adaptability as characteristics of AI-based\nsystems.\n2.",
  "2 Autonomy\nAI": "2 Autonomy\nAI-2.2.1 K",
  "2 Explain the relationship between autonomy and AI": "2 Explain the relationship between autonomy and AI-based systems.\n2.",
  "3 Evolution\nAI": "3 Evolution\nAI-2.3.1 K",
  "2 Explain the importance of managing evolution for AI": "2 Explain the importance of managing evolution for AI-based systems.\n2.",
  "4 Bias\nAI": "4 Bias\nAI-2.4.1 K",
  "2 Describe the different causes and types of bias found in AI": "2 Describe the different causes and types of bias found in AI-based systems.\n2.",
  "5 Ethics\nAI": "5 Ethics\nAI-2.5.1 K",
  "2 Discuss the ethical principles that should be respected in the development": "2 Discuss the ethical principles that should be respected in the development,\ndeployment and use of AI-based systems.\n2.",
  "6 Side Effects and Reward Hacking\nAI": "6 Side Effects and Reward Hacking\nAI-2.6.1 K",
  "2 Explain the occurrence of side effects and reward hacking in AI": "2 Explain the occurrence of side effects and reward hacking in AI-based systems.\n2.",
  "2 Explain how transparency": "2 Explain how transparency, interpretability and explainability apply to AI-based\nsystems.\n2.",
  "8 Safety and AI\nAI": "8 Safety and AI\nAI-2.8.1 K",
  "1 Recall the characteristics that make it difficult to use AI": "1 Recall the characteristics that make it difficult to use AI-based systems in safety-\nrelated applications.\nv1.",
  "1 Flexibility and Adaptability\nFlexibility and adaptability are closely related quality characteristics": "1 Flexibility and Adaptability\nFlexibility and adaptability are closely related quality characteristics. In this syllabus, flexibility is\nconsidered to be the ability of the system to be used in situations that were not part of the original\nsystem requirements, while adaptability is considered to be the ease with which the system can be\nmodified for new situations, such as different hardware and changing operational environments.\nBoth flexibility and adaptability are useful if:\n\u2022 the operational environment is not fully known when the system is deployed.\n\u2022 the system is expected to cope with new operational environments.\n\u2022 the system is expected to adapt to new situations.\n\u2022 the system must determine when it should change its behavior.\nSelf-learning AI-based systems are expected to demonstrate all of the above characteristics. As a\nconsequence, they must be adaptable and have the potential to be flexible.\nThe flexibility and adaptability requirements of an AI-based system should include details of any\nenvironment changes to which the system is expected to adapt. These requirements should also\nspecify constraints on the time and resources that the system can use to adapt itself (e.g., how long\ncan it take to adapt to recognizing a new type of object).\n2.",
  "2 Autonomy\nWhen defining autonomy": "2 Autonomy\nWhen defining autonomy, it is important to first recognize that a fully autonomous system would be\ncompletely independent of human oversight and control. In practice, full autonomy is not often\ndesired. For example, fully self-driving cars, which are popularly referred to as \u201cautonomous\u201d, are\nofficially classified as having \u201cfull driving automation\u201d [B07].\nMany consider autonomous systems to be \u201csmart\u201d or \u201cintelligent\u201d, which suggests they would include\nAI-based components to perform certain functions. For example, autonomous vehicles that need to\nbe situationally aware typically use several sensors and image processing to gather information about\nthe vehicle\u2019s immediate environment. Machine learning, and especially deep learning (see Section\n6.1), has been found to be the most effective approach to performing this function. Autonomous\nsystems may also include decision-making and control functions. Both of these can be effectively\nperformed using AI-based components.\nEven though some AI-based systems are considered to be autonomous, this does not apply to all AI-\nbased systems. In this syllabus, autonomy is considered to be the ability of the system to work\nindependently of human oversight and control for prolonged periods of time. This can help with\nidentifying the characteristics of an autonomous system that need to be specified and tested. For\nexample, the length of time an autonomous system is expected to perform satisfactorily without\nhuman intervention needs to be known. In addition, it is important to identify the events for which the\nautonomous system must give control back to its human controllers.\n2.",
  "3 Evolution\nIn this syllabus": "3 Evolution\nIn this syllabus, evolution is considered to be the ability of the system to improve itself in response to\nchanging external constraints. Some AI systems can be described as self-learning and successful\nself-learning AI-based systems need to incorporate this form of evolution.\nv1.",
  "4 Bias\nIn the context of AI": "4 Bias\nIn the context of AI-based systems, bias is a statistical measure of the distance between the outputs\nprovided by the system and what are considered to be \u201cfair outputs\u201d which show no favoritism to a\nparticular group. Inappropriate biases can be linked to attributes such as gender, race, ethnicity,\nsexual orientation, income level, and age. Cases of inappropriate bias in AI-based systems have\nbeen reported, for example, in systems used for making recommendations for bank lending, in\nrecruitment systems, and in judicial monitoring systems.\nBias can be introduced into many types of AI-based systems. For example, it is difficult to prevent the\nbias of experts being built-in to the rules applied by an expert system. However, the prevalence of ML\nsystems means that much of the discussion relating to bias takes place in the context of these\nsystems.\nML systems are used to make decisions and predictions, using algorithms which make use of\ncollected data, and these two components can introduce bias in the results:\n\u2022 Algorithmic bias can occur when the learning algorithm is incorrectly configured, for example,\nwhen it overvalues some data compared to others. This source of bias can be caused and\nmanaged by the hyperparameter tuning of the ML algorithms (see Section 3.2).\n\u2022 Sample bias can occur when the training data is not fully representative of the data space to\nwhich ML is applied.\nInappropriate bias is often caused by sample bias, but occasionally it can also be caused by\nalgorithmic bias.\n2.",
  "5 Ethics\nEthics is defined in the Cambridge Dictionary as": "5 Ethics\nEthics is defined in the Cambridge Dictionary as:\na system of accepted beliefs that control behavior, especially such a system based on morals\nAI-based systems with enhanced capabilities are having a largely positive effect on people\u2019s lives. As\nthese systems have become more widespread, concerns have been raised as to whether they are\nused in an ethical manner.\nv1.",
  "6 Side Effects and Reward Hacking\nSide effects and reward hacking can result in AI": "6 Side Effects and Reward Hacking\nSide effects and reward hacking can result in AI-based systems generating unexpected, and even\nharmful, results when the system attempts to meet its goals [B09].\nNegative side effects can result when the designer of an AI-based system specifies a goal that\n\u201cfocuses on accomplishing some specific tasks in the environment but ignores other aspects of the\n(potentially very large) environment, and thus implicitly expresses indifference over environmental\nvariables that might actually be harmful to change\u201d [B09]. For example, a self-driving car with a goal\nof travelling to its destination in \u201cas fuel-efficient and safe manner as possible\u201d may achieve the goal,\nbut with the side effect of the passengers becoming extremely annoyed at the excessive time taken.\nReward hacking can result from an AI-based system achieving a specified goal by using a \u201cclever\u201d or\n\u201ceasy\u201d solution that \u201cperverts the spirit of the designer\u2019s intent\u201d. Effectively, the goal can be gamed. A\nwidely used example of reward hacking is where an AI-based system is teaching itself to play an\narcade computer game. It is presented with the goal of achieving the \u201chighest score\u201d , and to do so it\nsimply hacks the data record that stores the highest score, rather than playing the game to achieve it.\n2.",
  "8 Safety and AI\nIn this syllabus": "8 Safety and AI\nIn this syllabus, safety is considered to be the expectancy that an AI-based system will not cause\nharm to people, property or the environment. AI-based systems may be used to make decisions that\naffect safety. For example, AI-based systems working in the fields of medicine, manufacturing,\ndefense, security, and transportation have the potential to affect safety.\nThe characteristics of AI-based systems that make it more difficult to ensure they are safe (e.g., do\nnot harm humans) include:\n\u2022 complexity\n\u2022 non-determinism\n\u2022 probabilistic nature\n\u2022 self-learning\n\u2022 lack of transparency, interpretability and explainability\n\u2022 lack of robustness\nThe challenges of testing several of these characteristics are covered in Chapter 8.\nv1.",
  "1 Forms of ML\nAI": "1 Forms of ML\nAI-3.1.1 K",
  "2 Describe classification and regression as part of supervised learning": "2 Describe classification and regression as part of supervised learning.\nAI-3.1.2 K",
  "2 Describe clustering and association as part of unsupervised learning": "2 Describe clustering and association as part of unsupervised learning.\nAI-3.1.3 K",
  "2 Describe reinforcement learning": "2 Describe reinforcement learning.\n3.",
  "2 ML Workflow\nAI": "2 ML Workflow\nAI-3.2.1 K",
  "2 Summarize the workflow used to create an ML system": "2 Summarize the workflow used to create an ML system.\n3.",
  "3 Selecting a Form of ML\nAI": "3 Selecting a Form of ML\nAI-3.3.1 K",
  "3 Given a project scenario": "3 Given a project scenario, identify an appropriate form of ML (from classification,\nregression, clustering, association, or reinforcement learning).\n3.",
  "4 Factors involved in ML Algorithm Selection\nAI": "4 Factors involved in ML Algorithm Selection\nAI-3.4.1 K",
  "2 Explain the factors involved in the selection of ML algorithms": "2 Explain the factors involved in the selection of ML algorithms.\n3.",
  "5 Overfitting and Underfitting\nAI": "5 Overfitting and Underfitting\nAI-3.5.1 K",
  "2 Summarize the concepts of underfitting and overfitting": "2 Summarize the concepts of underfitting and overfitting.\nHO-3.5.1 H",
  "0 Demonstrate underfitting and overfitting": "0 Demonstrate underfitting and overfitting.\nv1.",
  "1 Forms of ML\nML algorithms can be categorized as": "1 Forms of ML\nML algorithms can be categorized as:\n\u2022 supervised learning,\n\u2022 unsupervised learning, and\n\u2022 reinforcement learning.\n3.1.",
  "1 Supervised Learning\nIn this kind of learning": "1 Supervised Learning\nIn this kind of learning, the algorithm creates the ML model from labeled data during the training\nphase. The labeled data, which typically comprises pairs of inputs (e.g., an image of a dog and the\nlabel \u201cdog\u201d) is used by the algorithm to infer the relationship between the input data (e.g., images of\ndogs) and the output labels (e.g., \u201cdog\u201d and \u201ccat\u201d) during the training. During the ML model testing\nphase, a new set of unseen data is applied to the trained model to predict the output. The model is\ndeployed once the output accuracy level is satisfactory.\nProblems solved by supervised learning are divided into two categories:\n\u2022 Classification: This is when the problem requires an input to be classified into one of a few\npre-defined classes, classification is used. Face recognition or object detection in an image\nare examples of problems that use classification.\n\u2022 Regression: This is when the problem requires the ML model to predict a numeric output\nusing regression. Predicting the age of a person based on input data about their habits or\npredicting the future prices of stocks are examples of problems that use regression.\nNote that the term regression, as used in the context of a ML problem, is different to its use in other\n\u00ae\nISTQB syllabi, such as [I01], where regression is used to describe the problem of software\nmodifications causing change-related defects.\n3.1.",
  "2 Unsupervised Learning\nIn this kind of learning": "2 Unsupervised Learning\nIn this kind of learning, the algorithm creates the ML model from unlabeled data during the training\nphase. The unlabeled data is used by the algorithm to infer patterns in the input data during the\ntraining and assigns inputs to different classes, based on their commonalities. During the testing\nphase, the trained model is applied to a new set of unseen data to predict which classes the input\ndata should be assigned to. The model is deployed once the output accuracy level is considered to\nbe satisfactory.\nProblems solved by unsupervised learning are divided into two categories:\n\u2022 Clustering: This is when the problem requires the identification of similarities in input data\npoints that allows them to be grouped based on common characteristics or attributes. For\nexample, clustering is used to categorize different types of customers for the purpose of\nmarketing.\n\u2022 Association: This is when the problem requires interesting relationships or dependencies to\nbe identified among data attributes. For example, a product recommendation system may\nidentify associations based on customers\u2019 shopping behavior.\nv1.",
  "3 Reinforcement Learning\nReinforcement learning is an approach where the system ": "3 Reinforcement Learning\nReinforcement learning is an approach where the system (an \u201cintelligent agent\u201d) learns by interacting\nwith the environment in an iterative manner and thereby learns from experience. Reinforcement\nlearning does not use training data. The agent is rewarded when it makes a correct decision and\npenalized when it makes an incorrect decision.\nSetting up the environment, choosing the right strategy for the agent to meet the desired goal, and\ndesigning a reward function, are key challenges when implementing reinforcement learning.\nRobotics, autonomous vehicles, and chatbots are examples of applications that use reinforcement\nlearning.\n3.",
  "2 ML Workflow\nThe activities in the machine learning workflow are": "2 ML Workflow\nThe activities in the machine learning workflow are:\nUnderstand the Objectives\nThe purpose of the ML model to be deployed needs to be understood and agreed with the\nstakeholders to ensure alignment with business priorities. Acceptance criteria (including ML\nfunctional performance metrics \u2013 see Chapter 5) should be defined for the developed model.\nSelect a Framework\nA suitable AI development framework should be selected based on the objectives,\nacceptance criteria, and business priorities (see Section 1.5).\nSelect & Build the Algorithm\nAn ML algorithm is selected based on various factors including the objectives, acceptance\ncriteria, and the available data (see Section 3.4). The algorithm may be manually coded, but\nit is often retrieved from a library of pre-written code. The algorithm is then compiled to\nprepare for training the model, if required.\nPrepare & Test Data\nData preparation (see Section 4.1) comprises data acquisition, data pre-processing and\nfeature engineering. Exploratory data analysis (EDA) may be performed alongside these\nactivities.\nThe data used by the algorithm and model will be based on the objectives and is used by all\nthe activities in the \u201cmodel generation and test\u201d activity shown on Figure ",
  "1. For example": "1. For example, if\nthe system is a real-time trading system, the data will come from the trading market.\nThe data used to train, tune and test the model must be representative of the operational data\nthat will be used by the model. In some cases, it is possible to use pre-gathered datasets for\nthe initial training of the model (e.g., see Kaggle datasets [R16]). Otherwise, raw data\ntypically needs some pre-processing and feature engineering.\nTesting of the data and any automated data preparation steps needs to be performed. See\nSection 7.2.1 for more details on input data testing.\nTrain the Model\nThe selected ML algorithm uses training data to train the model.\nSome algorithms, such as those generating a neural network, read the training dataset\nseveral times. Each iteration of training on the training dataset is referred to as an epoch.\nv1.",
  "1.\nTest the Model\nOnce a model has been generated": "1.\nTest the Model\nOnce a model has been generated, (i.e., it has been trained, evaluated and tuned), it should\nbe tested against an independent test dataset set to ensure that the agreed ML functional\nperformance criteria are met (see Section 7.2.2). The functional performance measures from\ntesting are also compared with those from evaluation, and if the performance of the model\nwith independent data is significantly lower than during evaluation, it may be necessary to\nselect a different model.\nIn addition to functional performance tests, non-functional tests, such as for the time to train\nthe model, and the time and resource usage taken to provide a prediction, also need to be\nperformed. Typically, these tests are performed by the data engineer/scientist, but testers\nwith sufficient knowledge of the domain and access to the relevant resources can also\nperform these tests.\nDeploy the Model\nOnce model development is complete, as shown on Figure 1, the tuned model typically needs\nto be re-engineered for deployment along with its related resources, including the relevant\ndata pipeline. This is normally achieved through the framework. Targets might include\nembedded systems and the cloud, where the model can be accessed via a web API.\nv1.",
  "3 Selecting a Form of ML\nWhen selecting an appropriate ML approach": "3 Selecting a Form of ML\nWhen selecting an appropriate ML approach, the following guidelines apply:\n\u2022 There should be sufficient training and test data available for the selected ML approach.\n\u2022 For supervised learning, it is necessary to have properly labeled data.\n\u2022 If there is an output label, it may be supervised learning.\n\u2022 If the output is discrete and categorical, it may be classification.\n\u2022 If the output is numeric and continuous in nature, it may be regression.\n\u2022 If no output is provided in the given dataset, it may be unsupervised learning.\n\u2022 If the problem involves grouping similar data, it may be clustering.\n\u2022 If the problem involves finding co-occurring data items, it may be association.\n\u2022 Reinforcement learning is better suited to contexts in which there is interaction with the\nenvironment.\n\u2022 If the problem involves the notion of multiple states, and involves decisions at each state,\nthen reinforcement learning may be applicable.\n3.",
  "4 Factors Involved in ML Algorithm Selection\nThere is no definitive approach to selecting the optimal ML algorithm": "4 Factors Involved in ML Algorithm Selection\nThere is no definitive approach to selecting the optimal ML algorithm, ML model settings and ML\nmodel hyperparameters. In practice, this set is chosen based on a mix of the following factors:\n\u2022 The required functionality (e.g., whether the functionality is classification or prediction of a\ndiscrete value)\n\u2022 The required quality characteristics; such as\no accuracy (e.g., some models may be more accurate, but be slower)\no constraints on available memory (e.g., for an embedded system)\no the speed of training (and retraining) the model\no the speed of prediction (e.g., for real-time systems)\no transparency, interpretability and explainability requirements\nv1.",
  "5 Overfitting and Underfitting\n": "5 Overfitting and Underfitting\n3.5.",
  "1 Overfitting\nOverfitting occurs when the model fits too closely to a set of data points and fails to properly\ngeneralize": "1 Overfitting\nOverfitting occurs when the model fits too closely to a set of data points and fails to properly\ngeneralize. Such a model works very well with the data used to train it but can struggle to provide\naccurate predictions for new data. Overfitting can occur when the model tries to fit to every data\npoint, including those data points that may be described as noise or outliers. It can also occur when\ninsufficient data is provided in the training dataset.\n3.5.",
  "2 Underfitting\nUnderfitting occurs when the model is not sophisticated enough to accurately fit to the patterns in the\ntraining data": "2 Underfitting\nUnderfitting occurs when the model is not sophisticated enough to accurately fit to the patterns in the\ntraining data. Underfitting models tend to be too simplistic and can struggle to provide accurate\npredictions for both new data and data very similar to the training data. One cause of underfitting can\nbe a training dataset that does not contain features that reflect important relationships between inputs\nand outputs. It can also occur when the algorithm does not correctly fit the data (e.g., creating a\nlinear model for non-linear data).\n3.5.",
  "1 Data Preparation as part of the ML Workflow\nAI": "1 Data Preparation as part of the ML Workflow\nAI-4.1.1 K",
  "2 Describe the activities and challenges related to data preparation": "2 Describe the activities and challenges related to data preparation.\nHO-4.1.1 H",
  "2 Perform data preparation in support of the creation of an ML model": "2 Perform data preparation in support of the creation of an ML model.\n4.",
  "2 Contrast the use of training": "2 Contrast the use of training, validation and test datasets in the development of an ML\nmodel.\nHO-4.2.1 H",
  "2 Identify training and test datasets and create an ML model": "2 Identify training and test datasets and create an ML model.\n4.",
  "3 Dataset Quality Issues\nAI": "3 Dataset Quality Issues\nAI-4.3.1 K",
  "2 Describe typical dataset quality issues": "2 Describe typical dataset quality issues.\n4.",
  "4 Data quality and its effect on the ML model\nAI": "4 Data quality and its effect on the ML model\nAI-4.4.1 K",
  "2 Recognize how poor data quality can cause problems with the resultant ML model": "2 Recognize how poor data quality can cause problems with the resultant ML model.\n4.",
  "5 Data Labelling for Supervised Learning\nAI": "5 Data Labelling for Supervised Learning\nAI-4.5.1 K",
  "1 Recall the different approaches to the labelling of data in datasets for supervised\nlearning": "1 Recall the different approaches to the labelling of data in datasets for supervised\nlearning.\nAI-4.5.2 K",
  "1 Recall reasons for the data in datasets being mislabeled": "1 Recall reasons for the data in datasets being mislabeled.\nv1.",
  "1 Data Preparation as Part of the ML Workflow\nData preparation uses an average of ": "1 Data Preparation as Part of the ML Workflow\nData preparation uses an average of 43% of the ML workflow effort and is probably the most\nresource-intensive activity in the ML workflow. In comparison, model selection and building uses only\n17% [R17]. Data preparation forms part of the data pipeline, which takes in raw data and outputs\ndata in a form that can be used to both train an ML model and for prediction by a trained ML model.\nData preparation can be considered to comprise the following activities:\nData acquisition\n\u2022 Identification: The types of data to be used for training and predictions are identified. For\nexample, for a self-driving car, it could include the identification of the need for radar, video\nand laser imaging, detection, and ranging (LiDAR) data.\n\u2022 Gathering: The source of the data is identified and the means for collecting the data are\ndetermined. For example, this could include the identification of the International Monetary\nFund (IMF) as a source for financial data and the channels that will be used to submit the\ndata into the AI-based system.\n\u2022 Labelling: See Section 4.",
  "5.\nThe acquired data can be in various forms ": "5.\nThe acquired data can be in various forms (e.g., numerical, categorical, image, tabular, text, time-\nseries, sensor, geospatial, video, and audio).\nData pre-processing\n\u2022 Cleaning: Where incorrect data, duplicate data or outliers are identified, they are either\nremoved or corrected. In addition, data imputation may be used to replace missing data\nvalues with estimated or guessed values (e.g., using mean, median and mode values). The\nremoval or anonymization of personal information may also be performed.\n\u2022 Transformation: The format of the given data is changed (e.g., breaking an address held as a\nstring into its constituent parts, dropping a field holding a random identifier, converting\ncategorical data into numerical data, changing image formats). Some of the transformations\napplied on numerical data include scaling to ensure that the same range is used.\nStandardization, for example, rescales data to ensure it takes a mean of zero and a standard\ndeviation of one. This normalization ensures that the data has a range between zero and one.\n\u2022 Augmentation: This is used to increase the number of samples in a dataset. Augmentation\ncan also be used to include adversarial examples in the training data, providing robustness\nagainst adversarial attacks (see 9.1).\n\u2022 Sampling: This involves selection of some part of the total available dataset so that patterns\nin the larger dataset can be observed. This is typically done to reduce costs and the time\nneeded to create the ML model.\nNote that all pre-processing carries a risk that it may change useful valid data or add invalid data.\nFeature engineering\n\u2022 Feature selection: A feature is an attribute/property reflected in the data. Feature selection\ninvolves the selection of those features which are most likely to contribute to model training\nand prediction. In practice, it often includes the removal features that are not expected (or\nthat are not wanted) to have any effect on the resultant model. By removing irrelevant\ninformation (noise), feature selection can reduce overall training times, prevent overfitting\n(see Section 3.5.1), increase accuracy and make models more generalizable.\nv1.",
  "1 Challenges in Data Preparation\nSome of the challenges related to data preparation include": "1 Challenges in Data Preparation\nSome of the challenges related to data preparation include:\n\u2022 The need for knowledge of:\no the application domain.\no the data and its properties.\no the various techniques associated with data preparation.\n\u2022 The difficulty of getting high quality data from multiple sources.\n\u2022 The difficulty of automating the data pipeline, and ensuring that the production data pipeline is\nboth scalable and has reasonable performance efficiency (e.g., time needed to complete the\nprocessing of a data item).\n\u2022 The costs associated with data preparation.\n\u2022 Not giving sufficient priority to checking for defects introduced into the data pipeline during\ndata preparation.\n\u2022 The introduction of sample bias (see Section 2.4).\n4.1.",
  "3 Dataset Quality Issues\nTypical quality issues relating to the data in a dataset include": "3 Dataset Quality Issues\nTypical quality issues relating to the data in a dataset include, but are not limited to, those shown in\nthe following table:\nv1.",
  "4 Data Quality and its Effect on the ML Model\nThe quality of the ML model is highly dependent on the quality of the dataset from which it is created": "4 Data Quality and its Effect on the ML Model\nThe quality of the ML model is highly dependent on the quality of the dataset from which it is created.\nPoor quality data can result in both flawed models and flawed predictions.\nThe following categories of defects result from data quality issues:\n\u2022 Reduced accuracy: These defects are caused by data which is wrong, incomplete,\nmislabeled, insufficient, obsolete, irrelevant, and data which is not pre-processed. For\nexample, if the data was used to build a model of expected house prices, but the training data\ncontained little or no data on detached houses with conservatories, then the predicted prices\nfor this specific house type would probably be inaccurate.\n\u2022 Biased model: These defects are caused by data which is incomplete, unbalanced, unfair,\nlacking diversity, or duplicated. For example, if the data from a particular feature is missing\n(e.g., all the medical data for disease prediction is gathered from subjects of one particular\ngender), then this is likely to have an adverse effect on the resultant model (unless the model\nis only to be used to make predictions for that gender operationally).\n\u2022 Compromised model: These defects are due to data privacy and security restrictions . For\nexample, privacy issues in the data can lead to security vulnerabilities, which would enable\nattackers to reverse engineer information from the models and might subsequently cause\nleakage of personal information.\n4.",
  "5 Data Labelling for Supervised Learning\nData labelling is the enrichment of unlabeled ": "5 Data Labelling for Supervised Learning\nData labelling is the enrichment of unlabeled (or poorly labeled) data by adding labels, so it becomes\nsuitable for use in supervised learning. Data labelling is a resource-intensive activity that has been\nreported to use, on average, 25% of the time on ML projects [B11].\nIn its simplest form, data labelling can consist of putting images or text files in various folders, based\non their classes. For example, putting all text files of positive product reviews into one folder and all\nnegative reviews into another folder. Labelling objects in images by drawing rectangles around them\nis another common labelling technique, often known as annotation. More complex annotations could\nbe required for labelling 3D objects or for drawing bounding boxes around irregular objects. Data\nlabelling and annotation are typically supported by tools.\n4.5.",
  "1 Approaches to Data Labelling\nLabelling may be performed in a number of ways": "1 Approaches to Data Labelling\nLabelling may be performed in a number of ways:\n\u2022 Internal: The labelling is performed by developers, testers or a team within the organization\nwhich is set up for the labelling.\n\u2022 Outsourced: The labelling is done by an external specialist organization.\n\u2022 Crowdsourced: The labelling is performed by a large group of individuals. Due to the difficulty\nof managing the quality of the labelling, several annotators may be asked to label the same\ndata and a decision then taken on the label to be used.\n\u2022 AI-Assisted: AI-based tools are used to recognize and annotate data or to cluster similar data.\nThe results are then confirmed or perhaps supplemented (e.g., by modifying the bounding\nbox) by a human, as part of a two-step process.\nv1.",
  "2 Mislabeled Data in Datasets\nSupervised learning assumes that the data is correctly labeled by the data annotators": "2 Mislabeled Data in Datasets\nSupervised learning assumes that the data is correctly labeled by the data annotators. However, it is\nrare in practice for all items in a dataset to be labeled correctly. Data is mislabeled for the following\nreasons:\n\u2022 Random errors may be made by annotators (e.g., pressing the wrong button).\n\u2022 Systemic errors may be made, (e.g., where the labelers are given the wrong instructions or\npoor training).\n\u2022 Deliberate errors may be made by malicious data annotators.\n\u2022 Translation errors may take correctly labeled data in one language and mislabel it in another.\n\u2022 Where the choice is open to interpretation, subjective judgements made by data annotators\nmay lead to conflicting data labels from different annotators.\n\u2022 Lack of required domain knowledge may lead to incorrect labelling.\n\u2022 Complex classification tasks can result in more errors being made.\n\u2022 The tools used to support data labelling have defects that lead to incorrect labels.\n\u2022 ML-based approaches to labelling are probabilistic, and this can lead to some incorrect\nlabels.\nv1.",
  "1 Confusion Matrix\nAI": "1 Confusion Matrix\nAI-5.1.1 K",
  "3 Calculate the ML functional performance metrics from a given set of confusion matrix\ndata": "3 Calculate the ML functional performance metrics from a given set of confusion matrix\ndata.\n5.",
  "2 Contrast and compare the concepts behind the ML functional performance metrics for\nclassification": "2 Contrast and compare the concepts behind the ML functional performance metrics for\nclassification, regression and clustering methods.\n5.",
  "3 Limitations of ML Functional Performance Metrics\nAI": "3 Limitations of ML Functional Performance Metrics\nAI-5.3.1 K",
  "2 Summarize the limitations of using ML functional performance metrics to determine\nthe quality of the ML system": "2 Summarize the limitations of using ML functional performance metrics to determine\nthe quality of the ML system.\n5.",
  "4 Selecting ML Functional Performance Metrics\nAI": "4 Selecting ML Functional Performance Metrics\nAI-5.4.1 K",
  "4 Select appropriate ML functional performance metrics and": "4 Select appropriate ML functional performance metrics and/or their values for a given\nML model and scenario.\nHO-5.4.1 H",
  "2 Evaluate the created ML model using selected ML functional performance metrics\n": "2 Evaluate the created ML model using selected ML functional performance metrics\n5.",
  "5 Benchmark Suites for ML\nAI": "5 Benchmark Suites for ML\nAI-5.5.1 K",
  "2 Explain the use of benchmark suites in the context of ML\nv": "2 Explain the use of benchmark suites in the context of ML\nv1.",
  "1 Confusion Matrix\nIn a classification problem": "1 Confusion Matrix\nIn a classification problem, a model will rarely predict the results correctly all the time. For any such\nproblem, a confusion matrix can be created with the following possibilities:\nActual\nPositive Negative\nTrue Positive False Positive\nPositive\n(TP) (FP)\nPredicted\nFalse Negative True Negative\nNegative\n(FN) (TN)\nFigure 2: Confusion Matrix\nNote that the confusion matrix shown in Figure 2 may be presented differently but will always\ngenerate values for the four possible situations of true positive (TP), true negative (TN), false positive\n(FP) and false negative (FN).\nBased on the confusion matrix, the following metrics are defined:\n\u2022 Accuracy\nAccuracy = (TP + TN) / (TP +TN + FP + FN) * 100%\nAccuracy measures the percentage of all correct classifications.\n\u2022 Precision\nPrecision = TP / (TP + FP) * 100%\nPrecision measures the proportion of positives that were correctly predicted. It is a measure\nof how sure one can be about positive predictions.\n\u2022 Recall\nRecall = TP / (TP + FN) * 100%\nRecall (also known as sensitivity) measures the proportion of actual positives that were\npredicted correctly. It is a measure of how sure one can be about not missing any positives.\n\u2022 F1-score\nF1-score = 2* (Precision * Recall) / (Precision + Recall)\nF1-score is computed as the harmonic mean of precision and recall. It will have a value\nbetween zero and one. A score close to one indicates that false data has little influence on\nthe result. A low F1-score suggests that the model is poor at detecting positives.\nv1.",
  "3 Limitations of ML Functional Performance Metrics\nML functional performance metrics are limited to measuring the functionality of the model": "3 Limitations of ML Functional Performance Metrics\nML functional performance metrics are limited to measuring the functionality of the model, e.g., in\nterms of accuracy, precision, recall, MSE, AUC and the silhouette coefficient. They do not measure\nother non-functional quality characteristics, such as those defined in ISO 25010 [S06] (e.g.,\nperformance efficiency) and those described in Chapter 2, (e.g., explainability, flexibility, and\nautonomy). In this syllabus, the term \u201cML functional performance metrics\u201d is used because of the\nwidespread use of the term \u201cperformance metrics\u201d to refer to these functional metrics. Adding \u201cML\nfunctional\u201d highlights that these metrics are specific to machine learning and have no relationship to\nperformance efficiency metrics.\nv1.",
  "4 Selecting ML Functional Performance Metrics\nIt is not normally possible to build an ML model that achieves the highest score for all of the ML\nfunctional performance metrics generated from a confusion matrix": "4 Selecting ML Functional Performance Metrics\nIt is not normally possible to build an ML model that achieves the highest score for all of the ML\nfunctional performance metrics generated from a confusion matrix. Instead, the most appropriate ML\nfunctional performance metrics are selected as acceptance criteria, based on the expected use of the\nmodel (e.g., to minimize false positives, a high value of precision is required, whereas to minimize\nfalse negatives, the recall metric should be high). The following criteria can be used when selecting\nthe ML functional performance metrics described in Sections 5.1 and 5.2:\n\u2022 Accuracy: This metric is likely to be applicable if the datasets are symmetric (e.g., false\npositive and false negative counts and costs are similar). This metric becomes a poor choice\nif one class of data dominates over the others, in which case the F1-score should be\nconsidered.\n\u2022 Precision: This can be a suitable metric when the cost of false positives is high and\nconfidence in positive outcomes needs to be high. A spam filter, (where classifying an email\nas spam is considered positive), is an example where high precision is required, as putting\ntoo many emails in the spam folder that are not actually spam will not be acceptable to most\nusers. When the classifier deals with situations where a very large percentage of cases are\npositive, then using precision alone is unlikely to be a good choice.\n\u2022 Recall: When it is critical that positives should not be missed, then a high recall score is\nimportant. For example, missing any true positive results in cancer detection and marking\nthem as negative (i.e., no cancer detected) is likely to be unacceptable.\n\u2022 F1-score \u2013 F1-score is most useful when there is an imbalance in the expected classes and\nwhen the precision and recall are of similar importance.\nIn addition to the above metrics, several metrics are described in Section 5.",
  "2. These may be\napplicable for given ML problems": "2. These may be\napplicable for given ML problems, for example:\n\u2022 The AUC for the ROC curve may be used for supervised classification problems.\n\u2022 MSE and R-squared may be used for supervised regression problems.\n\u2022 Inter-cluster metrics, intra-cluster metrics and the silhouette coefficient may be used for\nunsupervised clustering problems.\nv1.",
  "5 Benchmark Suites for ML\nNew AI technologies such as new datasets": "5 Benchmark Suites for ML\nNew AI technologies such as new datasets, algorithms, models, and hardware are released regularly,\nand it can be difficult to determine the relative efficacy of each new technology.\nTo provide objective comparisons between these different technologies, industry-standard ML\nbenchmark suites are available. These cover a wide range of application areas, and provide tools to\nevaluate hardware platforms, software frameworks and cloud platforms for AI and ML performance.\nML benchmark suites can provide various measures, including training times (e.g., how fast a\nframework can train an ML model using a defined training dataset to a specified target quality metric,\nsuch as 75% accuracy), and inference times (e.g., how fast a trained ML model can perform\ninference).\nML benchmark suites are provided by several different organizations, such as:\n\u2022 MLCommons [R18]: This is a not-for-profit organization formed in 2020 and previously named\nML Perf, which provides benchmarks for software frameworks, AI-specific processors and ML\ncloud platforms.\n\u2022 DAWNBench [R19]: This is an ML benchmark suite from Stanford University.\n\u2022 MLMark [R20]: This is an ML benchmark suite designed to measure the performance and\naccuracy of embedded inference from the Embedded Microprocessor Benchmark\nConsortium.\nv1.",
  "1 Neural Networks\nAI": "1 Neural Networks\nAI-6.1.1 K",
  "2 Explain the structure and function of a neural network including a DNN": "2 Explain the structure and function of a neural network including a DNN.\nHO-6.1.1 H",
  "1 Experience the implementation of a perceptron": "1 Experience the implementation of a perceptron.\n6.",
  "2 Coverage Measures for Neural Networks\nAI": "2 Coverage Measures for Neural Networks\nAI-6.2.1 K",
  "2 Describe the different coverage measures for neural networks": "2 Describe the different coverage measures for neural networks.\nv1.",
  "1 Neural Networks\nArtificial neural networks were initially intended to mimic the functioning of the human brain": "1 Neural Networks\nArtificial neural networks were initially intended to mimic the functioning of the human brain, which can\nbe thought of as many connected biological neurons. The single-layer perceptron is one of the first\nexamples of the implementation of an artificial neural network and comprises a neural network with\njust one layer (i.e., a single neuron). It can be used for supervised learning of classifiers, which\ndecide whether an input belongs to a specific class or not.\nMost current neural networks are considered to be deep neural networks because they comprise\nseveral layers and can be considered as multi-layer perceptrons (see Figure 3).\nFigure ",
  "3 Structure of a deep neural network\nA deep neural network comprises three types of layers": "3 Structure of a deep neural network\nA deep neural network comprises three types of layers. The input layer receives inputs, for example\npixel values from a camera. The output layer provides results to the outside world. This might be, for\nexample, a value signifying the likelihood that the input image is a cat. Between the input and output\nlayers are hidden layers made up of artificial neurons, which are also known as nodes. The neurons\nin one layer are connected to each of the neurons in the next layer and there may be different\nnumbers of neurons in each successive layer. The neurons perform computations and pass\ninformation across the network from the input neurons to the output neurons.\nv1.",
  "4 Computation performed by each neuron\nAs shown in Figure ": "4 Computation performed by each neuron\nAs shown in Figure 4, the computation performed by each neuron (except those in the input layer)\ngenerates what is known as the activation value. This value is calculated by running a formula (the\nactivation function) that receives as input the activation values from all the neurons in the previous\nlayer, the weights assigned to the connections between the neurons (these weights change as the\nnetwork learns), and the individual bias of each neuron. Note that this bias is a preset constant value\nand is not related to the bias considered earlier in Section 2.4). Running different activation functions\ncan result in different activation values being calculated. These values are typically centered around\nzero and have a range between -1 (meaning that the neuron is \u201cdisinterested\u201d) and +1 (meaning that\nthe neuron is \u201cvery interested\u201d).\nWhen training the neural network, each neuron is preset to a bias value and the training data is\npassed through the network, with each neuron running the activation function, to eventually generate\nan output. The generated output is then compared with the known correct result (labeled data is used\nin this example of supervised learning). The difference between the actual output and the known\ncorrect result is then fed back through the network to modify the values of the weights on the\nconnections between the neurons in order to minimize this difference. As more training data is fed\nthrough the network, the weights are gradually adjusted as the network learns. Ultimately, the outputs\nproduced are considered good enough to end training.\n6.1.",
  "2 Coverage Measures for Neural Networks\nAchieving white": "2 Coverage Measures for Neural Networks\nAchieving white-box test coverage criteria (e.g., statement, branch, modified condition/decision\ncoverage (MC/DC) [I01] is mandatory for compliance with some safety-related standards [S07] when\nusing traditional imperative source code, and is recommended by many test practitioners for other\ncritical applications. Monitoring and improving coverage supports the design of new test cases,\nleading to increased confidence in the test object.\nUsing such measures for measuring the coverage of neural networks provides little value as the same\ncode tends to be run each time the neural network is executed. Instead, coverage measures have\nbeen proposed based on the coverage of the structure of the neural network itself, and more\nspecifically, the neurons within it. Most of these measures are based on the activation values of the\nneurons.\nCoverage for neural networks is a new research area. Academic papers have only been published\nsince 2017, and as such, there is little objective evidence available (e.g., duplicated research results)\nthat show the proposed measures are effective. It should be noted, however, that despite statement\nand decision coverage having been used for over 50 years, there is also little objective evidence of\ntheir relative effectiveness, even though they have been mandated for measuring coverage of\nsoftware in safety-related applications, such as medical devices and avionics systems.\nThe following coverage criteria for neural networks have been proposed and applied by researchers\nto a variety of applications:\n\u2022 Neuron coverage: Full neuron coverage requires that each neuron in the neural network\nachieves an activation value greater than zero [B12]. This is very easy to achieve in practice\nand research has shown that almost 100% coverage is achieved with very few test cases on\na variety of deep neural networks. This coverage measure may be most useful as an alarm\nsignal when it is not achieved.\n\u2022 Threshold coverage: Full threshold coverage requires that each neuron in the neural network\nachieves an activation value greater than a specified threshold. The researchers who created\nthe DeepXplore framework actually suggested that neuron coverage should be measured\nbased on the activation value exceeding a threshold which would change based on the\nsituation. They performed their research with a threshold of 0.75 when they reported\nefficiently finding thousands of incorrect corner case behaviors using this white-box approach.\nThis type of coverage has been renamed here to distinguish it more easily from neuron\ncoverage with a threshold set to zero, as some other researchers use the term \u201cneuron\ncoverage\u201d to mean neuron coverage with a threshold of zero.\n\u2022 Sign-Change coverage: To achieve full sign-change coverage, test cases need to cause each\nneuron to achieve both positive and negative activation values [B13].\n\u2022 Value-Change coverage: To achieve full value-change coverage, test cases need to cause\neach neuron to achieve two activation values, where the difference between the two values\nexceeds some chosen value [B13].\n\u2022 Sign-Sign coverage: This coverage considers pairs of neurons in adjacent layers and the sign\ntaken by their activation values. For a pair of neurons to be considered covered, a test case\nneeds to show that changing the sign of a neuron in the first layer causes the neuron in the\nsecond layer to change its sign, while the signs of all other neurons in the second layer\nremain unchanged [B13]. This is a similar concept to MC/DC coverage for imperative source\ncode.\nResearchers have reported on further coverage measures based on layers (although simpler than\nsign-sign coverage), and a successful approach using nearest neighbor algorithms to identify\nv1.",
  "2 Explain how system specifications for AI": "2 Explain how system specifications for AI-based systems can create challenges in\ntesting.\n7.",
  "2 Describe how AI": "2 Describe how AI-based systems are tested at each test level\n7.",
  "1 Recall those factors associated with test data that can make testing AI": "1 Recall those factors associated with test data that can make testing AI-based\nsystems difficult.\n7.",
  "2 Explain automation bias and how this affects testing": "2 Explain automation bias and how this affects testing.\n7.",
  "5 Documenting an ML Model\nAI": "5 Documenting an ML Model\nAI-7.5.1 K",
  "2 Describe the documentation of an AI component and understand how documentation\nsupports the testing of AI": "2 Describe the documentation of an AI component and understand how documentation\nsupports the testing of AI-based systems.\n7.",
  "6 Testing for Concept Drift\nAI": "6 Testing for Concept Drift\nAI-7.6.1 K",
  "2 Explain the need for frequently testing the trained model to handle concept drift": "2 Explain the need for frequently testing the trained model to handle concept drift.\n7.",
  "7 Selecting a Test Approach for an ML System\nAI": "7 Selecting a Test Approach for an ML System\nAI-7.7.1 K",
  "4 For a given scenario determine a test approach to be followed when developing an\nML system": "4 For a given scenario determine a test approach to be followed when developing an\nML system.\nv1.",
  "1 Input Data Testing\nThe objective of input data testing is to ensure that the data used by the system for training and\nprediction is of the highest quality ": "1 Input Data Testing\nThe objective of input data testing is to ensure that the data used by the system for training and\nprediction is of the highest quality (see Section 4.3). It includes the following:\n\u2022 Reviews\n\u2022 Statistical techniques (e.g., testing data for bias)\n\u2022 EDA of the training data\n\u2022 Static and dynamic testing of the data pipeline\nThe data pipeline typically comprises several components performing data preparation (see Section\n4.1), and the testing of these components includes both component testing and integration testing.\nThe data pipeline for training may be quite different from the data pipeline used to support operational\nprediction. For training, the data pipeline can be considered a prototype, compared to the fully\nengineered, automated version used operationally. For this reason, the testing of these two versions\nof the data pipeline may be quite distinct. However, testing the functional equivalence of the two\nversions should also be considered.\n7.2.",
  "2 ML Model Testing\nThe objective of ML model testing is to ensure that the selected model meets any performance criteria\nthat may have been specified": "2 ML Model Testing\nThe objective of ML model testing is to ensure that the selected model meets any performance criteria\nthat may have been specified. This includes:\n\u2022 ML functional performance criteria (see Sections 5.1 and 5.2)\n\u2022 ML non-functional acceptance criteria that are appropriate for the model in isolation, such as\nspeed of training, speed of prediction, computing resources used, adaptability, and\ntransparency.\nML model testing also aims to determine that the choice of ML framework, algorithm, model, model\nsettings and hyperparameters is as close to optimal as possible. Where appropriate, ML model\ntesting may also include testing to achieve white-box coverage criteria (see Section 6.2). The selected\nmodel is later integrated with other components, AI and non-AI.\n7.2.",
  "3 Component Testing\nComponent testing is a conventional test level which is applicable to any non": "3 Component Testing\nComponent testing is a conventional test level which is applicable to any non-model components,\nsuch as user interfaces and communication components.\n7.2.",
  "4 Component Integration Testing\nComponent integration testing is a conventional test level which is conducted to ensure that the\nsystem components ": "4 Component Integration Testing\nComponent integration testing is a conventional test level which is conducted to ensure that the\nsystem components (both AI and non-AI) interact correctly. It tests that the inputs from the data\npipeline are received as expected by the model, and that any predictions produced by the model are\nexchanged with the relevant system components (e.g., the user interface) and used correctly by them.\nWhere AI is provided as a service (see Section 1.7), it is normal to perform API testing of the provided\nservice as part of component integration testing.\n7.2.",
  "5 System Testing\nSystem testing is a conventional test level which is conducted to ensure that the complete system of\nintegrated components ": "5 System Testing\nSystem testing is a conventional test level which is conducted to ensure that the complete system of\nintegrated components (both AI and non-AI) performs as expected, from both functional and non-\nv1.",
  "6 Acceptance Testing\nAcceptance testing is a conventional test level and is used to determine whether the complete system\nis acceptable to the customer": "6 Acceptance Testing\nAcceptance testing is a conventional test level and is used to determine whether the complete system\nis acceptable to the customer. For AI-based systems, the definition of acceptance criteria can be\nchallenging (see Section 8.8). Where AI is provided as a service (see Section 1.7), acceptance\ntesting may be needed to determine the suitability of the service for the intended system and whether,\nfor example, ML functional performance criteria have been sufficiently achieved.\n7.",
  "5 Documenting an AI Component\nThe typical content for the documentation of an AI component includes": "5 Documenting an AI Component\nThe typical content for the documentation of an AI component includes:\n\u2022 General: Identifiers, description, developer details, hardware requirements, license details,\nversion, date and point of contact.\n\u2022 Design: Assumptions and technical decisions.\n\u2022 Usage: Primary and secondary use cases, typical users, approach to self-learning, known\nbias, ethical issues, safety issues, transparency, decision thresholds, platform and concept\ndrift.\n\u2022 Datasets: Features, collection, availability, pre-processing requirements, use, content,\nlabelling, size, privacy, security, bias/fairness and restrictions/constraints.\n\u2022 Testing: Test dataset (description and availability), independence of testing, test results,\ntesting approach for robustness, explainability, concept drift and portability.\n\u2022 Training and ML Functional Performance: ML algorithm, weights, validation dataset, selection\nof ML functional performance metrics, thresholds for ML functional performance metrics, and\nactual ML functional performance metrics.\nClear documentation helps improve the testing by providing transparency on the implementation of\nthe AI-based system. The key areas of documentation that are important to testing are:\n\u2022 The purpose of the system, and the specification of functional and non-functional\nrequirements. These types of documentation typically form part of the test basis.\n\u2022 Architectural and design information, outlining how the different AI and non-AI components\ninteract. This supports the identification of integration testing objectives, and may provide a\nbasis for white-box testing of the system structure.\n\u2022 The specification of the operating environment. This is required when testing the autonomy,\nflexibility and adaptability of the system.\n\u2022 The source of any input data, including associated metadata. This needs to be clearly\nunderstood when testing the following aspects:\no Functional correctness of untrustworthy inputs\nv1.",
  "6 Testing for Concept Drift\nThe operational environment can change over time without the trained model changing\ncorrespondingly": "6 Testing for Concept Drift\nThe operational environment can change over time without the trained model changing\ncorrespondingly. This phenomenon is known as concept drift and typically causes the outputs of the\nmodel to become increasingly less accurate and less useful. For example, the impact of marketing\ncampaigns may result in a change in the behavior of potential customers over a period of time. Such\nchanges could be seasonal or abrupt changes due to cultural, moral or societal changes which are\nexternal to the system. An example of such an abrupt change is the impact of the COVID-19\npandemic and its effect on the accuracy of the models used for sales projections and stock markets.\nSystems that may be prone to concept drift should be regularly tested against their agreed ML\nfunctional performance criteria, to ensure that any occurrences of concept drift are detected soon\nenough for the problem to be mitigated. Typical mitigations may include retiring the system or re-\ntraining the system. In the case of re-training, this would be performed with up-to-date training data\nand followed by confirmation testing, regression testing, and possibly a form of A/B testing (see\nSection 9.4), where the updated B-system must outperform the original A-system.\n7.",
  "7 Selecting a Test Approach for an ML System\nAn AI": "7 Selecting a Test Approach for an ML System\nAn AI-based system will typically include both AI and non-AI components. The test approach is\nbased on a risk analysis for such a system and will include both conventional testing as well as more\nspecialized testing to address those factors specific to AI components and AI-based systems.\nThe following list provides some typical risks and corresponding mitigations, specific to ML systems.\nNote that this list only provides a limited set of examples and that there are many more risks specific\nto ML systems that require mitigation through testing.\nRisk Aspect Description and possible Mitigations\nData quality may be lower This risk may become an issue in several ways, each of which may\nthan expected. be prevented in different manners (see Section 4.4). Common\nmitigations include the use of reviews, EDA and dynamic testing.\nThe operational data pipeline This risk can be partly mitigated by the dynamic testing of the\nmay be faulty. individual pipeline components and the integration testing of the\ncomplete pipeline\nThe ML workflow used to This risk could be due to the following:\ndevelop the model may be\n\u2022 A lack of up-front agreement on the ML workflow to be\nsub-optimal.\nfollowed\nv1.",
  "2 Explain the challenges in testing created by the self": "2 Explain the challenges in testing created by the self-learning of AI-based systems.\n8.",
  "2 Describe how autonomous AI": "2 Describe how autonomous AI-based systems are tested\n8.",
  "2 Explain how to test for bias in an AI": "2 Explain how to test for bias in an AI-based system.\n8.",
  "2 Explain the challenges in testing created by the probabilistic and non": "2 Explain the challenges in testing created by the probabilistic and non-deterministic\nnature of AI-based systems.\n8.",
  "2 Explain the challenges in testing created by the complexity of AI": "2 Explain the challenges in testing created by the complexity of AI-based systems.\n8.",
  "2 Describe how the transparency": "2 Describe how the transparency, interpretability and explainability of AI-based systems\ncan be tested.\nHO-8.6.1 H",
  "2 Use a tool to show how explainability can be used by testers": "2 Use a tool to show how explainability can be used by testers.\n8.",
  "2 Explain the challenges in creating test oracles resulting from the specific\ncharacteristics of AI": "2 Explain the challenges in creating test oracles resulting from the specific\ncharacteristics of AI-based systems.\n8.",
  "8 Test Objectives and Acceptance Criteria\nAI": "8 Test Objectives and Acceptance Criteria\nAI-8.8.1 K",
  "4 Select appropriate test objectives and acceptance criteria for the AI": "4 Select appropriate test objectives and acceptance criteria for the AI-specific quality\ncharacteristics of a given AI-based system.\nv1.",
  "8 Test Objectives and Acceptance Criteria\nTest objectives and acceptance criteria for a system need to be based on the perceived product risks": "8 Test Objectives and Acceptance Criteria\nTest objectives and acceptance criteria for a system need to be based on the perceived product risks.\nThese risks can often be identified from an analysis of the required quality characteristics. The quality\nv1.",
  "1 Adversarial Attacks and Data Poisoning\nAI": "1 Adversarial Attacks and Data Poisoning\nAI-9.1.1 K",
  "2 Explain how the testing of ML systems can help prevent adversarial attacks and data\npoisoning": "2 Explain how the testing of ML systems can help prevent adversarial attacks and data\npoisoning.\n9.",
  "2 Pairwise Testing\nAI": "2 Pairwise Testing\nAI-9.2.1 K",
  "2 Explain how pairwise testing is used for AI": "2 Explain how pairwise testing is used for AI-based systems.\nLO-9.2.1 H",
  "2 Apply pairwise testing to derive and execute test cases for an AI": "2 Apply pairwise testing to derive and execute test cases for an AI-based system.\n9.",
  "2 Explain how back": "2 Explain how back-to-back testing is used for AI-based systems.\n9.4 A/B Testing\nAI-9.4.1 K",
  "2 Explain how A": "2 Explain how A/B testing is applied to the testing of AI-based systems.\n9.",
  "5 Metamorphic Testing\nAI": "5 Metamorphic Testing\nAI-9.5.1 K",
  "3 Apply metamorphic testing for the testing of AI": "3 Apply metamorphic testing for the testing of AI-based systems.\nHO-9.5.1 H",
  "2 Apply metamorphic testing to derive test cases for a given scenario and execute\nthem": "2 Apply metamorphic testing to derive test cases for a given scenario and execute\nthem.\n9.",
  "2 Explain how experience": "2 Explain how experience-based testing can be applied to the testing of AI-based\nsystems.\nHO-9.6.1 H",
  "2 Apply exploratory testing to an AI": "2 Apply exploratory testing to an AI-based system.\n9.",
  "4 For a given scenario": "4 For a given scenario, select appropriate test techniques when testing an AI-based\nsystem.\nv1.",
  "1 Adversarial Attacks and Data Poisoning\n": "1 Adversarial Attacks and Data Poisoning\n9.1.",
  "1 Adversarial Attacks\nAn adversarial attack is where an attacker subtly perturbs valid inputs that are passed to the trained\nmodel to cause it to provide incorrect predictions": "1 Adversarial Attacks\nAn adversarial attack is where an attacker subtly perturbs valid inputs that are passed to the trained\nmodel to cause it to provide incorrect predictions. These perturbed inputs, known as adversarial\nexamples, were first noticed with spam filters, which could be tricked by slightly modifying a spam\nemail without losing readability. Recently, they have become more associated with image classifiers.\nBy simply changing a few pixels which are invisible to the human eye, it is possible to persuade a\nneural network to change its image classification to a very different object and with a high degree of\nconfidence.\nAdversarial examples are generally transferable [B17], which means that an adversarial example\nwhich causes one ML system to fail will often cause another ML system to fail that is trained to\nperform the same task. Even when the second ML system has been trained with different data and is\nbased on different architectures, it is often still prone to failure with the same adversarial examples.\nWhite-box adversarial attacks are where the attacker knows which algorithm was used to train the\nmodel and also which model settings and parameters were used (there is a reasonable level of\ntransparency). The attacker uses this knowledge to generate adversarial examples by, for example,\nmaking small perturbations in inputs and monitoring which ones cause large changes to the model\noutputs.\nBlack-box adversarial attacks involve the attacker exploring the model to determine its functionality\nand then building a duplicate model that provides similar functionality. The attacker then uses a\nwhite-box approach to identify adversarial examples for this duplicate model. As adversarial\nexamples are generally transferable, the same adversarial examples will normally also work on the\noriginal model.\nIf it is not possible to create a duplicate model, it may be possible to use high-volume automated\ntesting to discover different adversarial examples and observe the results.\nAdversarial testing simply involves performing adversarial attacks with the purpose of identifying\nvulnerabilities so that preventative measures can be taken to protect against future failures. Identified\nadversarial examples are added to the training data so that the model is trained to correctly recognize\nthem.\n9.1.",
  "2 Data Poisoning\nData poisoning attacks are where an attacker manipulates the training data to achieve one of two\nresults": "2 Data Poisoning\nData poisoning attacks are where an attacker manipulates the training data to achieve one of two\nresults. The attacker may insert backdoors or neural network trojans to facilitate future intrusions, or\nmore often, they will use corrupted training data (e.g., mislabeled data) to induce the trained model to\nprovide incorrect predictions.\nPoisoning attacks may be targeted with the aim of causing the ML system to misclassify in specific\nsituations. They may also be indiscriminate, such as with a denial-of-service attack. A well-known\nexample of a poisoning attack was the corruption of the Microsoft Tay chatbot, whereby a relatively\nsmall number of harmful Twitter conversations trained the system through feedback to provide tainted\nconversations in the future. A commonly used form of data poisoning attack uses the false reporting\nof millions of spam emails as not being spam in an attempt to skew spam filtering software. An area\nof concern with data poisoning is the potential for public, widely used AI datasets to become\npoisoned.\nv1.",
  "2 Pairwise Testing\nThe number of parameters of interest for an AI": "2 Pairwise Testing\nThe number of parameters of interest for an AI-based system can be extremely high, especially when\nthe system uses big data or interacts with the outside world, such as a self-driving car. Exhaustive\ntesting would require all possible combinations of these parameters set to all possible values to be\ntested. However, since this would result in a practically infinite number of tests, test techniques are\nused to select a subset that can be run in the limited time available.\nWhere it is possible to combine numerous parameters, each of which may have many discrete\nvalues, combinatorial testing can be applied to significantly reduction the number of test cases\nrequired, ideally without compromising the defect detection capability of the test suite. There are\nseveral combinatorial testing techniques (see [I02] and [S08]). However, in practice, pairwise testing\nis the most widely used technique because it is easy to understand, has ample tool support. In\naddition, research has shown that most defects are caused by interactions involving few parameters\n[B33].\nIn practice, even the use of pairwise testing can result in extensive test suites for some systems, and\nthe use of automation and virtual test environments (see Section 10.2) often becomes necessary to\nallow the necessary number of tests to be run. For example, when considering self-driving cars, high-\nlevel test scenarios for system testing need to exercise both the different environments in which the\ncars are expected to operate and the various vehicle functions. Thus, the parameters would need to\ninclude the range of environment constraints (e.g., road types and surfaces, weather and traffic\nconditions, and visibility) and the various self-driving functions (e.g., adaptive cruise control, lane\nkeeping assistance, and lane change assistance). In addition to these parameters, inputs from\nsensors could be considered at varying levels of effectiveness (e.g., inputs from a video camera will\ndegrade as a journey progress and it gets dirtier).\nResearch is currently unclear on the necessary level of rigor that would be required for the use of\ncombinatorial testing with safety-critical AI-based systems such as self-driving cars. Even though\npairwise testing may not be sufficient), it is known that the approach is effective at finding defects.\n9.2.",
  "5. Whenever the system\nis updated": "5. Whenever the system\nis updated, A/B testing is used to check that the updated variant performs as well as, or better than\nthe previous variant. Such an approach can be used for a simple classifier, but can also be used for\ntesting far more complex systems. For example, an update to improve the effectiveness of a smart\ncity transport routing system can also be tested using A/B testing (e.g., comparing average commute\ntimes for two variants of the system on consecutive weeks).\nA/B testing can also be used to test self-learning systems. When the system makes a change,\nautomated tests are run, and the resultant system characteristics are compared with those before the\nchange was made. If the system is improved, then the change is accepted, otherwise the system\nreverts to its previous state.\nOne major difference between A/B testing and back-to-back testing relates to the use of A/B testing to\ncompare two variants of the same system and the use of back-to-back testing to detect defects.\n9.",
  "1998. It differs from traditional test techniques\nin that the expected results of the follow": "1998. It differs from traditional test techniques\nin that the expected results of the follow-up test cases are not described in terms of absolute values,\nbut are relative to the expected results in the source test case. It is based on an easily understood\nconcept, can be applied by testers with little experience of applying the technique but who understand\nthe application domain, and has similar costs compared to traditional techniques. It is also effective at\nrevealing defects, with research showing that only three to six diverse MRs can reveal over 90% of\nthe defects that could be detected using techniques based on a traditional test oracle [B19]. It is\npossible to automatically generate follow-up test cases from well-specified MRs and a source test\ncase. However, commercial tools are not currently available, although Google is already applying\nautomated MT to test Android graphics drivers using the GraphicsFuzz tool, which has been open\nsourced (see [R23]).\nv1.",
  "1. Feature expectations are captured in a schema": "1. Feature expectations are captured in a schema.\nv1.",
  "2. All features are beneficial": "2. All features are beneficial.\n",
  "3. No feature": "3. No feature\u2019s cost is too much.\n",
  "4. Features adhere to metalevel requirements": "4. Features adhere to metalevel requirements.\n",
  "5. The data pipeline has appropriate privacy controls": "5. The data pipeline has appropriate privacy controls.\n",
  "6. New features can be added quickly": "6. New features can be added quickly.\n",
  "7. All input feature code is tested": "7. All input feature code is tested.\nModel Development:\n",
  "1. Model specs are reviewed and submitted": "1. Model specs are reviewed and submitted.\n",
  "2. Offline and online metrics correlate": "2. Offline and online metrics correlate.\n",
  "3. All hyperparameters have been tuned": "3. All hyperparameters have been tuned.\n",
  "4. The impact of model staleness is known": "4. The impact of model staleness is known.\n",
  "5. A simpler model is not better": "5. A simpler model is not better.\n",
  "6. Model quality is sufficient on important data slices": "6. Model quality is sufficient on important data slices.\n",
  "7. The model is tested for considerations of inclusion": "7. The model is tested for considerations of inclusion.\nML Infrastructure:\n",
  "1. Training is reproducible": "1. Training is reproducible.\n",
  "2. Model specs are unit tested": "2. Model specs are unit tested.\n",
  "3. The ML pipeline is integration tested": "3. The ML pipeline is integration tested.\n",
  "4. Model quality is validated before serving": "4. Model quality is validated before serving.\n",
  "5. The model is debuggable": "5. The model is debuggable.\n",
  "6. Models are canaried before serving": "6. Models are canaried before serving.\n",
  "7. Serving models can be rolled back\nMonitoring Tests": "7. Serving models can be rolled back\nMonitoring Tests:\n",
  "1. Dependency changes result in notification": "1. Dependency changes result in notification.\n",
  "2. Data invariants hold for inputs": "2. Data invariants hold for inputs.\n",
  "3. Training and serving are not skewed": "3. Training and serving are not skewed.\n",
  "4. Models are not too stale": "4. Models are not too stale.\n",
  "5. Models are numerically stable": "5. Models are numerically stable.\n",
  "6. Computing performance has not regressed": "6. Computing performance has not regressed.\n",
  "7. Prediction quality has not regressed": "7. Prediction quality has not regressed.\n9.6.",
  "2 Describe the main factors that differentiate the test environments for AI": "2 Describe the main factors that differentiate the test environments for AI-based\nsystems from those required for conventional systems.\n10.",
  "2 Describe the benefits provided by virtual test environments in the testing of AI": "2 Describe the benefits provided by virtual test environments in the testing of AI-based\nsystems.\nv1.",
  "1 AI Technologies for Testing\nAI": "1 AI Technologies for Testing\nAI-11.1.1 K",
  "2 Categorize the AI technologies used in software testing": "2 Categorize the AI technologies used in software testing.\nHO-11.1.1 H",
  "2 Discuss": "2 Discuss, using examples, those activities in testing where AI is less likely to be used.\n11.",
  "2 Using AI to Analyze Reported Defects\nAI": "2 Using AI to Analyze Reported Defects\nAI-11.2.1 K",
  "2 Explain how AI can assist in supporting the analysis of new defects": "2 Explain how AI can assist in supporting the analysis of new defects.\n11.",
  "3 Using AI for Test Case Generation\nAI": "3 Using AI for Test Case Generation\nAI-11.3.1 K",
  "2 Explain how AI can assist in test case generation": "2 Explain how AI can assist in test case generation.\n11.",
  "4 Using AI for the Optimization of Regression Test Suites\nAI": "4 Using AI for the Optimization of Regression Test Suites\nAI-11.4.1 K",
  "2 Explain how AI can assist in optimization of regression test suites\n": "2 Explain how AI can assist in optimization of regression test suites\n11.",
  "5 Using AI for Defect Prediction\nAI": "5 Using AI for Defect Prediction\nAI-11.5.1 K",
  "2 Explain how AI can assist in defect prediction": "2 Explain how AI can assist in defect prediction.\nHO-11.5.1 H",
  "2 Implement a simple AI": "2 Implement a simple AI-based defect prediction system.\n11.",
  "6 Using AI for Testing User Interfaces\nAI": "6 Using AI for Testing User Interfaces\nAI-11.6.1 K",
  "2 Explain the use of AI in testing user interfaces\nv": "2 Explain the use of AI in testing user interfaces\nv1.",
  "1 AI Technologies for Testing\nSeveral AI technologies are listed in Section": "1 AI Technologies for Testing\nSeveral AI technologies are listed in Section1.4, all of which can be used to support some specific\naspect of software testing. According to Harman [B24], the software engineering community uses\nthree broad areas of AI technologies:\n\u2022 Fuzzy logic and probabilistic methods: These involve the use of AI techniques to handle real\nworld problems which are themselves probabilistic. For example, AI can be used to analyze\nand predict possible system failures using Bayesian techniques. These may estimate the\nlikelihood of components or functions failing, or reflect the potentially random nature of human\ninteractions with the system.\n\u2022 Classification, learning and prediction: This can be used for various use cases such as\npredicting costs as part of project planning or of predicting defects. As embodied by ML, this\narea is used for many software testing tasks, including defect management (see Section\n11.2), defect prediction (see Section 11.5), and user interface testing (see Section 11.6).\n\u2022 Computational search and optimization techniques: These can be used to solve optimization\nproblems using a computational search of potentially large and complex search spaces (e.g.,\nusing search algorithms). Examples include generating test cases (see Section 11.3),\nidentifying the smallest number of test cases that achieves a given coverage criterion, and\noptimizing regression test cases (see Section 11.4).\nThe above categorization is necessarily broad, as there is considerable overlap between the testing\ntasks that can be implemented by AI and the different AI technologies. It is also just one\ncategorization, and others could be created that may be equally valid.\n11.1.",
  "2 Using AI to Analyze Reported Defects\nReported defects are usually categorized": "2 Using AI to Analyze Reported Defects\nReported defects are usually categorized, prioritized, and any duplicates identified. This activity is\noften referred to as defect triage or analysis and is intended to optimize the elapsed time spent in\ndefect resolution. AI can be used to support this activity in various ways, such as:\n\u2022 Categorization: NLP [B25] can be used to analyze text within defect reports and extract\ntopics, such as the area of affected functionality, that can then be provided alongside other\nmeta data to clustering algorithms, such as k-nearest neighbors or support vector machines.\nv1.",
  "3 Using AI for Test Case Generation\nThe use of AI to generate tests can be a very effective technique for quickly create testing assets and\nmaximizing coverage ": "3 Using AI for Test Case Generation\nThe use of AI to generate tests can be a very effective technique for quickly create testing assets and\nmaximizing coverage (e.g., code or requirements coverage). The basis for generating these tests\nincludes the source code, the user interface, and a machine-readable test model. Some tools also\nbase tests on the observation of the low-level behavior of the system through instrumentation or\nthrough log files [B27].\nHowever, unless a test model that defines required behaviors is used as the basis of the tests, this\nform of test generation generally suffers from a test oracle problem because the AI-based tool does\nnot know what the expected results should be for a given set of test data. One solution is to use\nback-to-back testing if a suitable system is available to use as a pseudo-oracle (see Section 9.3).\nAlternatively, tests could be run with the expected result that neither an \u201capplication not responding\u201d\nnor a system crash occurred, or other similar simple failure indicators.\nResearch comparing AI-based test generation tools with similar non-AI fuzz testing tools shows that\nthe AI-based tools can achieve equivalent levels of coverage and find more defects while reducing the\naverage sequence of steps needed to cause a failure from an average of around 15,000 steps to\naround 100 steps. This makes debugging far easier [B27].\n11.",
  "4 Using AI for the Optimization of Regression Test Suites\nAs changes are made to a system": "4 Using AI for the Optimization of Regression Test Suites\nAs changes are made to a system, new tests are created, executed and become candidates for a\nregression test suite. To prevent regression test suites from growing too large, they should be\nfrequently optimized to select, prioritize and even augment test cases to create a more effective and\nefficient regression test suite.\nAn AI-based tool can perform optimization of the regression test suite by analyzing, for example, the\ninformation from previous test results, associated defects, and the latest changes that have been\nmade, such as features which are broken more frequently and which tests exercise code impacted by\nrecent changes.\nResearch shows that reductions of 50% in the size of a regression test suite can be achieved while\nstill detecting most defects [B28], and reductions of 40% in the test execution duration can be reached\nwithout significant reduction in fault detection for continuous integration testing [B29].\n11.",
  "5 Using AI for Defect Prediction\nDefect prediction can be used to predict whether a defect is present": "5 Using AI for Defect Prediction\nDefect prediction can be used to predict whether a defect is present, how many defects are present,\nor whether defects can be found. This capability depends on the sophistication of the tool used.\nv1.",
  "6 Using AI for Testing User Interfaces\n": "6 Using AI for Testing User Interfaces\n11.6.",
  "2 Using AI to Test the GUI\nML models can be used to determine the acceptability of user interface screens ": "2 Using AI to Test the GUI\nML models can be used to determine the acceptability of user interface screens (e.g., by using\nheuristics and supervised learning). Tools based on these models can identify incorrectly rendered\nelements, determine whether some objects are inaccessible or hard to detect, and detect various\nother issues with the visual appearance of the GUI.\nWhile image recognition is one form of computer vision algorithm, other forms of AI-based computer\nvision can be used to compare images (e.g., screenshots) to identify unintended changes to the\nlayout, the size, position, color, font or other visible attributes of objects. The results of these\ncomparisons can be used to support regression testing to check that changes to the test object have\nnot adversely affected the user interface.\nThe technology for checking the acceptability of screens can be combined with comparison tools to\ncreate more sophisticated AI-based regression testing tools that are capable of advising whether\ndetected user interface changes are likely to be acceptable to users, or whether these changes\nshould be flagged for checking by a human. Such AI-based tools can also be used to support testing\nfor compatibility on different browsers, devices or platforms aimed at checking that the user interface\nfor the same application works correctly on various browsers/devices/platforms.\nv1.",
  "12 References\n": "12 References\n12.",
  "1 Standards\n": "1 Standards\n[S01] ISO/IEC TR 29119-11:2020, Software and systems engineering \u2014 Software testing\n\u2014 Part ",
  "11 Guidelines on the testing of AI": "11 Guidelines on the testing of AI-based systems\n[S02] DIN SPEC 92001-1, Artificial Intelligence - Life Cycle Processes and Quality\nRequirements - Part 1: Quality Meta Model, https://www.din.de/en/wdc-\nbeuth:din21:303650673 (accessed May 2021).\n[S03] DIN SPEC 92001-2, Artificial Intelligence - Life Cycle Processes and Quality\nRequirements - Part 2: Technical and Organizational Requirements,\nhttps://www.din.de/en/innovation-and-research/din-spec-en/projects/wdc-\nproj:din21:298702628 (accessed May 2021).\n[S04] ISO 26262 - https://www.iso.org/standard/68383.html (accessed May 2021)\n[S05] ISO/PAS 21448:2019, Road vehicles \u2013 Safety of the intended functionality (SOTIF) -\nhttps://www.iso.org/standard/70939.html (accessed May 2021)\n[S06] ISO/IEC 25010:2011, Systems and software engineering \u2014 Systems and software\nQuality Requirements and Evaluation (SQuaRE) \u2014 System and software quality\nmodels, 2011.\n[S07] ISO 26262-6:2018 - Road vehicles - Functional safety - Part 6: Product development\nat the software level.\n[S08] ISO/IEC/IEEE 29119-4:2015, Software and systems engineering \u2014 Software testing\n\u2014 Part 4: Test techniques.\n\u00ae\n12.",
  "2 ISTQB Documents\n": "2 ISTQB Documents\n\u00ae\n[I01] ISTQB Certified Tester Foundation Level Syllabus, Version 2018 V3.1\nhttps://www.istqb.org/downloads/category/2-foundation-level-documents.html\n(accessed May 2021).\n\u00ae\n[I02] ISTQB Certified Tester Advanced Level Test Analyst Syllabus, Version 3.1, section\n3.2.6\nhttps://www.istqb.org/downloads/category/75-advanced-level-test-analyst-v3-1.html\n(accessed August 2021).\n\u00ae\n[I03] ISTQB Certified Tester AI Testing, Overview of Syllabus, Version 1.0\nv1.",
  "3 Books and Articles\n": "3 Books and Articles\n[B01] Cadwalladr, Carole (2014). \"Are the robots about to rise? Google's new director of\nengineering thinks so\u2026\" The Guardian. Guardian News and Media Limited.,\nhttps://www.theguardian.com/technology/2014/feb/22/robots-google-ray-kurzweil-\nterminator-singularity-artificial-intelligence (accessed May 2021).\n[B02] Stuart Russell and Peter Norvig, Artificial Intelligence: A Modern Approach, 4th\nEdition, Pearson, 2020.\n[B03] M. Davies et al., \u201cAdvancing Neuromorphic Computing With Loihi: A Survey of\nResults and Outlook,\u201d Proceedings of the IEEE, vol. 109, no. 5, pp. 911\u2013934, May\n2021, doi: 10.1109/JPROC.2021.3067593.\n[B04] Chris Wiltz, Can Apple Use Its Latest AI Chip for More Than Photos?, Electronics &\nTest, Artificial Intelligence, https://www.designnews.com/electronics-test/can-apple-\nuse-its-latest-ai-chip-more-photos/153617253461497 (accessed May 2021).\n[B05] HUAWEI Reveals the Future of Mobile AI at IFA 2017, Huawei Press Release,\nhttps://consumer.huawei.com/en/press/news/2017/ifa2017-kirin970/ (accessed May\n2021).\n[B06] REGULATION (EU) 2016/",
  "679 OF THE EUROPEAN PARLIAMENT AND OF THE\nCOUNCIL on the protection of natural persons with regard to the processing of\npersonal data and on the free movement of such data": "679 OF THE EUROPEAN PARLIAMENT AND OF THE\nCOUNCIL on the protection of natural persons with regard to the processing of\npersonal data and on the free movement of such data, and repealing Directive\n95/46/EC (General Data Protection Regulation), April 2016,https://eur-\nlex.europa.eu/eli/reg/2016/679/oj (accessed May 2021)\n[B07] Taxonomy and Definitions for Terms Related to Driving Automation Systems for On-\nRoad Motor Vehicles J3016_201806, SAE,\n,https://www.sae.org/standards/content/j3016_201806/ (accessed May 2021).\n[B08] G",
  "20 Ministerial Statement on Trade and Digital Economy": "20 Ministerial Statement on Trade and Digital Economy: Annex. Available from:\nhttps://www.mofa.go.jp/files/000486596.pdf (accessed May 2021).\n[B09] Concrete Problems in AI Safety, Dario Amodei (Google Brain), Chris Olah (Google\nBrain), Jacob Steinhardt (Stanford University), Paul Christiano (UC Berkeley), John\nSchulman (OpenAI), Dan Man\u00b4e (Google Brain), March 2016.\nhttps://arxiv.org/pdf/1606.06565 (accessed May 2021).\n[B10] Explainable AI: the basics, Policy briefing, Issued: November ",
  "2019 DES": "2019 DES6051, ISBN:\n978-1-78252-433-5, The Royal Society.\n[B11] The Ultimate Guide to Data Labeling for Machine Learning,\nwww.cloudfactory.com/data-labeling-guide (accessed May 2021).\nv1.",
  "1. Edition": "1. Edition, Addison-Wesley Professional, 2009.\n[B21] L. Wilkinson, A. Anand, and R. Grossman. High-dimensional visual analytics:\nInteractive exploration guided by pairwise views of point distributions. Visualization\nand Computer Graphics, IEEE Transactions on, 12(6):1363\u20131372, 2006,\nhttps://www.cs.uic.edu/~wilkinson/Publications/sorting.pdf (accessed May 2021).\n[B22] Ryan Hafen and Terence Critchlow, EDA and ML \u2013 A Perfect Pair for Large-Scale\nData Analysis, IEEE 27th International Symposium on Parallel and Distributed\nProcessing, 2013,\nhttps://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6651091 (accessed May\n2021).\n[B23] Breck, Eric, Shanqing Cai, Eric Nielsen, Michael Salib, and D. Sculley., The ML Test\nScore: A Rubric for ML Production Readiness and Technical Debt Reduction, IEEE\nInternational Conference on Big Data (Big Data), 2017,\nhttps://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8258038 (accessed May\n2021).\nv1.",
  "6. IEEE": "6. IEEE, June 2012,\nhttps://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6227961 (accessed May\n2021).\n[B25] Nilambri et al, A Survey on Automated Duplicate Detection in a Bug Repository,\nInternational Journal of Engineering Research & Technology (IJERT), 2014,\nhttps://www.ijert.org/research/a-survey-on-automated-duplicate-detection-in-a-bug-\nrepository-IJERTV3IS041769.pdf (accessed May 2021)\n[B26] Kim, D.; Wang, X.; Kim, S.; Zeller, A.; Cheung, S.C.; Park, S. (2011). \u201cWhich\nCrashes Should I Fix First? Predicting Top Crashes at an Early Stage to Prioritize\nDebugging Efforts,\u201d in the IEEE Transactions on Software Engineering, volume 37,\nhttps://ieeexplore.ieee.org/document/5711013 (accessed May 2021).\n[B27] Mao et al, Sapienz: multi-objective automated testing for Android applications,\nProceedings of the 25th International Symposium on Software Testing and Analysis,\nJuly 2016, http://www0.cs.ucl.ac.uk/staff/K.Mao/archive/p_issta16_sapienz.pdf\n(accessed May 2021).\n[B28] Rai et al, Regression Test Case Optimization Using Honey Bee Mating Optimization\nAlgorithm with Fuzzy Rule Base, World Applied Sciences Journal 31 (4): 654-662,\n2014,\nhttps://www.researchgate.net/publication/336133351_Regression_Test_Case_Optimi\nzation_Using_Honey_Bee_Mating_Optimization_Algorithm_with_Fuzzy_Rule_Base\n(accessed May 2021).\n[B29] Dusica Marijan, Arnaud Gotlieb, Marius Liaaen. A learning algorithm for optimizing\ncontinuous integration development and testing practice, Journal of Software :\nPractice and Experience, Nov 2018.\n[B30] Tosun et al, AI-Based Software Defect Predictors: Applications and Benefits in a\nCase Study, Proceedings of the Twenty-Second Innovative Applications of Artificial\nIntelligence Conference (IAAI-10), 2010.\n[B31] Kim et al, Predicting Faults from Cached History, 29th International Conference on\nSoftware Engineering (ICSE\u201907), 2007.\n[B32] Nagappan ",
  "2008 Nagappan et al": "2008 Nagappan et al, The Influence of Organizational Structure on\nSoftware Quality: An Empirical Case Study, Proceedings of the 30th international\nconference on Software engineering (ICSE\u201908), May 2008.\n[B33] Kuhn et al, Software Fault Interactions and Implications for Software Testing, IEEE\nTransactions on Software Engineering vol. 30, no. 6, (June 2004) pp. 418-421.\nv1.",
  "4 Other References\nThe following references point to information available on the Internet": "4 Other References\nThe following references point to information available on the Internet. Even though these references\n\u00ae\nwere checked at the time of publication, the ISTQB cannot be held responsible if the references are\nno longer available.\n[R01] Wikipedia contributors, \"AI effect,\" Wikipedia, https://en.wikipedia.org/wiki/AI_effect\n(accessed May 2021).\n[R02] https://mxnet.apache.org/ (accessed May 2021).\n[R03] https://docs.microsoft.com/en-us/cognitive-toolkit/ (accessed May 2021).\n[R04] IBM Watson, https://www.ibm.com/watson/ai-services\n[R05] https://www.tensorflow.org/ (accessed May 2021).\n[R06] https://keras.io/ (accessed May 2021).\n[R07] https://pytorch.org/ (accessed May 2021).\n[R08] https://scikit-learn.org/stable/whats_new/v0.23.html (accessed May 2021).\n[R09] NVIDIA VOLTA, https://www.nvidia.com/en-us/data-center/volta-gpu-architecture/\n(accessed May 2021).\n[R10] Cloud TPU, https://cloud.google.com/tpu/ (accessed May 2021).\n[R11] Edge TPU, https://cloud.google.com/edge-tpu/ (accessed May 2021).\n[R12] Intel\u00ae Nervana\u2122 Neural Network processors deliver the scale and efficiency\ndemanded by deep learning model evolution, https://www.intel.ai/nervana-nnp/\n(accessed May 2021).\n[R13] The Evolution of EyeQ, https://www.mobileye.com/our-technology/evolution-eyeq-\nchip/ (accessed May 2021).\n[R14] ImageNet - http://www.image-net.org/ (accessed May 2021).\n[R15] Google\u2019s BERT - https://github.com/google-research/bert (accessed May 2021).\n[R16] https://www.kaggle.com/datasets (accessed May 2021).\n[R17] https://www.kaggle.com/paultimothymooney/2018-kaggle-machine-learning-data-\nscience-survey (accessed May 2021).\n[R18] MLCommons - https://mlcommons.org/ (accessed May 2021).\n[R19] DAWNBench \u2013 https://dawn.cs.stanford.edu/benchmark (accessed May 2021).\n[R20] MLMark \u2013 https://www.eembc.org/mlmark (accessed May 2021).\nv1.",
  "1\nAI that exhibits intelligent behaviour comparable to a human across the\nfull range of cognitive abilities ": "1\nAI that exhibits intelligent behaviour comparable to a human across the\nfull range of cognitive abilities (ISO/IEC TR 29119-11)\ngeneral AI\nSynonym: strong AI\nThe European Union (EU) regulation on data protection and privacy\nGeneral Data Protection\nthat applies to the data of citizens of the EU and the European\nRegulation (GDPR)\nEconomic Area\nAn application-specific integrated circuit designed to manipulate and\ngraphical processing unit\nalter memory to accelerate the creation of images in a frame buffer\n(GPU)\nintended for output to a display device\nThe information provided by direct observation and measurement that\nground truth\nis known to be real or true\nA parameter used to either control the training of an ML model or to\nhyperparameter\nset the configuration of an ML model\nThe activity of determining the optimal hyperparameters based on\nhyperparameter tuning\nparticular goals\nA type of bias that causes a system to produce results that lead to\ninappropriate bias\nadverse effects for a particular group\nAn autonomous program which directs its activity towards achieving\nintelligent agent\ngoals using observations and actions\ninter-cluster metric A metric that measures the similarity of data points in different clusters\nThe level of understanding how the underlying AI technology works\ninterpretability\n(ISO/IEC TR 29119-11)\nintra-cluster metric A metric that measures the similarity of data points within a cluster\nAn approach to classification that estimates the likelihood of group\nk-nearest neighbor membership for a data point dependent on the group membership of\nthe data points nearest to it\nA program that produces an ML model based on the properties of the\nlearning algorithm\ntraining dataset\nv1.",
  "15 Index\nA": "15 Index\nA/B testing 73, 74, 77 encryption 56\nacceptance testing 56 error guessing 75\nadaptability 22, 23, 24, 54, 55, 58 experience-based testing 34, 75, 78\nadversarial attack 20, 37, 71 explainability 22, 26, 27, 45, 56, 57, 66, 67,\n69, 80\nadversarial example 37, 71, 77\nexploratory testing 66, 75, ",
  "76\nAPI testing ": "76\nAPI testing 55\nflexibility 23\nattacker 20, 41, 71\nfollow-up test case 74, 75\nautomation bias 57\nfunctional correctness 58\navailability 18, 57, 81\nfunctional suitability 68\nback-to-back testing 68, 73, 74, 77, 84\nfuzz testing 84\nbias 24, 26, 40, 50, 64\ngraphical user interface 82, 85, 86\nblack-box testing 66\ninput data testing 55\nchecklist-based testing 75\ninstallability 74\nclassification 29, 33, 44, 45\nintegration testing 55, 58\nclustering 29, 33, 45, 47\ninterpretability 26, 67\ncombinatorial testing 72\nmachine learning 46, 57\ncompatibility 68\nmaintainability 68\ncomplexity 27, 66\nmetamorphic relation 74, 75\ncomponent 55, 56, 59\nmetamorphic testing 74, 75\ncomponent integration testing ",
  "55\nML algorithms ": "55\nML algorithms 29\ncomponent testing ",
  "55\nML functional performance criteria ": "55\nML functional performance criteria 55, 56,\nconfirmation testing 65\n58, 59, 60\ncontinuous testing ",
  "63\nML functional performance metrics ": "63\nML functional performance metrics 18, 30,\ncoverage 55, 78, 83, 84 31, 43, 45, 46, 57, 69, 73\ndata poisoning 71, ",
  "77 ML model ": "77 ML model 17, 18, 19, 30, 33, 37, 39, 41, 46\ndata privacy ",
  "41 ML model testing ": "41 ML model testing 29, 55, 56\ndebugging ",
  "84 ML workflow ": "84 ML workflow 30, 33, 37, 39\ndecision tree 16, 31 neural network 49, 50, 51, 71\ndenial-of-service 71 neuron coverage 51\ndynamic testing 55, 66 non-functional requirement 56, 58, 68, 73\neffectiveness 20, 24, 72, 73 non-functional test 31, 74\nefficiency 24, 78 operational environment 24, 56, 58, 63, 64,\n68, 80\nv1."
}